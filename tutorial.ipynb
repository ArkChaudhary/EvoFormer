{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4417735",
   "metadata": {},
   "source": [
    "NexusFlow: Complete Tutorial & Demo\n",
    "Multi-Transformer Framework for Tabular Ecosystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54af82bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis notebook provides a comprehensive walkthrough of NexusFlow, demonstrating:\\n1. Project initialization and setup\\n2. Data preparation and configuration\\n3. Model training with both synthetic and real data\\n4. Model evaluation and inference\\n5. Advanced features and introspection\\n\\nNexusFlow is inspired by AlphaFold 2's collaborative intelligence approach,\\nusing multiple specialized transformers that communicate via cross-attention.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook provides a comprehensive walkthrough of NexusFlow, demonstrating:\n",
    "1. Project initialization and setup\n",
    "2. Data preparation and configuration\n",
    "3. Model training with both synthetic and real data\n",
    "4. Model evaluation and inference\n",
    "5. Advanced features and introspection\n",
    "\n",
    "NexusFlow is inspired by AlphaFold 2's collaborative intelligence approach,\n",
    "using multiple specialized transformers that communicate via cross-attention.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b7770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d491ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12943de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó NexusFlow - Multi-Transformer Framework for Tabular Data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó NexusFlow - Multi-Transformer Framework for Tabular Data\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc133eaa",
   "metadata": {},
   "source": [
    "# üöÄ Section 1: Environment Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5d9b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NexusFlow v0.1.0 loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# First, let's verify our NexusFlow installation and setup\n",
    "try:\n",
    "    # Add the src directory to path if running from project root\n",
    "    if 'src' not in sys.path:\n",
    "        src_path = Path.cwd() / 'src'\n",
    "        if src_path.exists():\n",
    "            sys.path.insert(0, str(src_path))\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Running from non-project directory. Ensure nexusflow is installed.\")\n",
    "    \n",
    "    # Import core NexusFlow modules\n",
    "    from src.nexusflow import __version__\n",
    "    from src.nexusflow.config import load_config_from_file, ConfigModel\n",
    "    from src.nexusflow.project_manager import ProjectManager\n",
    "    from src.nexusflow.trainer.trainer import Trainer\n",
    "    from src.nexusflow.api.model_api import load_model, ModelAPI\n",
    "    from src.nexusflow.data.ingestion import load_datasets, align_datasets\n",
    "    \n",
    "    print(f\"‚úÖ NexusFlow v{__version__} loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üí° Make sure you're running from the project root or have installed nexusflow\")\n",
    "    print(\"   Try: pip install -e .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0774c57",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Section 2: Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1068d7d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_project_initialization():\n",
    "    \"\"\"Demonstrate NexusFlow project structure creation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèóÔ∏è  PROJECT INITIALIZATION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize project manager\n",
    "    pm = ProjectManager()\n",
    "    \n",
    "    # Create a demo project (use force to overwrite if exists)\n",
    "    project_name = \"nexusflow_demo\"\n",
    "    \n",
    "    try:\n",
    "        project_path = pm.init_project(project_name, force=True)\n",
    "        print(f\"‚úÖ Demo project created at: {project_path}\")\n",
    "        \n",
    "        # Show the created directory structure\n",
    "        demo_path = Path(project_name)\n",
    "        if demo_path.exists():\n",
    "            print(\"\\nüìÅ Project Structure:\")\n",
    "            for item in sorted(demo_path.rglob(\"*\")):\n",
    "                if item.is_dir():\n",
    "                    level = len(item.relative_to(demo_path).parts)\n",
    "                    indent = \"  \" * (level - 1)\n",
    "                    print(f\"{indent}üìÅ {item.name}/\")\n",
    "                elif item.name.endswith(('.yaml', '.yml', '.md', '.txt')):\n",
    "                    level = len(item.relative_to(demo_path).parts)\n",
    "                    indent = \"  \" * (level - 1)\n",
    "                    print(f\"{indent}üìÑ {item.name}\")\n",
    "        \n",
    "        return demo_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Project creation failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9332a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 03:29:32.172\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\configs\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\datasets\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\models\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.187\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\notebooks\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\results\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.196\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mDirectory created: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\\src\u001b[0m\n",
      "\u001b[32m2025-08-25 03:29:32.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.project_manager\u001b[0m:\u001b[36minit_project\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mProject initialized at: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèóÔ∏è  PROJECT INITIALIZATION DEMO\n",
      "============================================================\n",
      "‚úÖ Demo project created at: C:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\nexusflow_demo\n",
      "\n",
      "üìÅ Project Structure:\n",
      "üìÅ configs/\n",
      "üìÅ datasets/\n",
      "üìÅ models/\n",
      "üìÅ notebooks/\n",
      "üìÅ results/\n",
      "üìÅ src/\n"
     ]
    }
   ],
   "source": [
    "# Run the demo\n",
    "demo_project = demo_project_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16758e1d",
   "metadata": {},
   "source": [
    "# üìä Section 3: Synthetic Data Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96904996",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_synthetic_training_demo():\n",
    "    \"\"\"Demonstrate training with synthetic data.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä SYNTHETIC DATA TRAINING DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a synthetic data configuration\n",
    "    synthetic_config = {\n",
    "        'project_name': 'synthetic_demo',\n",
    "        'primary_key': 'id',\n",
    "        'target': {\n",
    "            'target_table': 'table_a.csv',\n",
    "            'target_column': 'label'\n",
    "        },\n",
    "        'architecture': {\n",
    "            'refinement_iterations': 2,\n",
    "            'global_embed_dim': 32\n",
    "        },\n",
    "        'datasets': [\n",
    "            {'name': 'table_a.csv', 'transformer_type': 'standard', 'complexity': 'small'},\n",
    "            {'name': 'table_b.csv', 'transformer_type': 'standard', 'complexity': 'small'}\n",
    "        ],\n",
    "        'training': {\n",
    "            'use_synthetic': True,\n",
    "            'synthetic': {\n",
    "                'n_samples': 512,\n",
    "                'feature_dim': 8\n",
    "            },\n",
    "            'batch_size': 32,\n",
    "            'epochs': 3,\n",
    "            'optimizer': {'name': 'adam', 'lr': 1e-3},\n",
    "            'split_config': {\n",
    "                'test_size': 0.2,\n",
    "                'validation_size': 0.2,\n",
    "                'randomize': True\n",
    "            }\n",
    "        },\n",
    "        'mlops': {\n",
    "            'logging_provider': 'stdout',\n",
    "            'experiment_name': 'synthetic_demo'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create config object\n",
    "    config = ConfigModel.model_validate(synthetic_config)\n",
    "    print(\"‚úÖ Configuration created for synthetic data training\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    try:\n",
    "        trainer = Trainer(config, work_dir=\".\")\n",
    "        print(\"‚úÖ Trainer initialized\")\n",
    "        \n",
    "        # Run sanity check\n",
    "        print(\"\\nüîç Running sanity check...\")\n",
    "        trainer.sanity_check()\n",
    "        print(\"‚úÖ Sanity check passed\")\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nüöÄ Starting training...\")\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ Training completed!\")\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(\"\\nüìà Evaluating model...\")\n",
    "        metrics = trainer.evaluate()\n",
    "        \n",
    "        if metrics:\n",
    "            print(\"\\nüìä Final Evaluation Results:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        return config, trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee62507",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 03:32:40.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_file_logging\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mFile logging enabled: results\\logs\\training_synthetic_demo.log\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:40.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mEnhanced Trainer initialized (device=cpu)\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:40.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mUsing synthetic data mode\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:40.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m187\u001b[0m - \u001b[1mSynthetic data: 2 datasets √ó 8 features\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:40.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mNexusFormer initialized: 2 encoders, 2 iterations\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SYNTHETIC DATA TRAINING DEMO\n",
      "============================================================\n",
      "‚úÖ Configuration created for synthetic data training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 03:32:54.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_params\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLogging parameters: {'model_name': 'NexusFormer', 'input_dims': [8, 8], 'embed_dim': 32, 'refinement_iterations': 2, 'num_parameters': 46145, 'learning_rate': 0.001, 'optimizer': 'adam', 'batch_size': 32, 'epochs': 3}\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:54.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mModel initialized: 46145 parameters\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:54.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mRunning comprehensive sanity checks...\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([2, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([2, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.015\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.772422\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.814881\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.034\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.510359\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.036\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.542525\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([2])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mModel forward pass: output_shape=torch.Size([2])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_dataloaders\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSynthetic DataLoaders: train=12 val=3 test=3 batches\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mSample batch: features=[torch.Size([32, 8]), torch.Size([32, 8])] targets=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.074\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.076\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.085\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.719646\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.114388\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.095\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.096\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.535641\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.098\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.100\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.762696\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.102\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.118\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.119\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.132\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.722616\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.135\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.185578\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.139\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.144\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.146\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.512194\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.883484\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.158\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.168\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.174\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([12, 32])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n",
      "\n",
      "üîç Running sanity check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 03:32:55.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.185\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.537369\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.190\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.113461\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.192\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.199\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.389723\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.204\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.851439\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.206\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([12, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([12])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mInitial validation metrics: {'accuracy': 0.5921052694320679, 'val_loss': 0.6927823225657145}\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m372\u001b[0m - \u001b[1mAll sanity checks passed!\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mStarting enhanced training: 3 epochs\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_dataloaders\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mSynthetic DataLoaders: train=12 val=3 test=3 batches\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.236\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.250\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.789800\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.117986\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.263\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.573174\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.812659\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.307\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.309\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.717870\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.319\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.166600\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.550684\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.873963\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.335\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.356\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.367\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.782464\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.369\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.241658\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.376\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.596125\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.912349\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.408\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0002\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sanity check passed\n",
      "\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 03:32:55.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.414\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.726055\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.426\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.229769\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.429\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.431\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.501103\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.435\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.895782\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.754864\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.480\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.188930\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.484\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.530954\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.492\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.813898\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.497\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.785801\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.533\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.187233\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.531363\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.544\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.888230\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.546\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.567\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.577\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.583\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.662790\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.586\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.124871\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.590972\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.599\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.877241\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.621\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.629\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.718006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.634\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.636\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.261316\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.531760\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.799685\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.668\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.677\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.682623\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.149787\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.516482\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.704\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.812269\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.708\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.712\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.756\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.759\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.723045\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.164749\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.770\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.774\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.509867\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.839624\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.802\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.803\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.804\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.808\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.782722\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.811\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.218281\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.813\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.815\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.525698\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.868785\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=-0.0008\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.802954\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.853\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.049233\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.856\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.860\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.500559\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.862\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.844284\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([6, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([6])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.898\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.902\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.906\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.908\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.769353\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.910\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.119371\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.912\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.914\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.497861\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.733156\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.922\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.926\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.927\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.930\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.931\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.732527\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.229682\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.941\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.943\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.442094\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.869885\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.948\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.949\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.952\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=-0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.959\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.579067\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.153744\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.967\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.340685\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.864079\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([12, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([12])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_metrics\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mMetrics (step 1): train_loss=0.688897 epoch=1.000000 epoch_time=0.753045 accuracy=0.618421 val_loss=0.688052\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mNew best model at epoch 1: 0.688052\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:55.999\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_save_checkpoint\u001b[0m:\u001b[36m477\u001b[0m - \u001b[34m\u001b[1mCheckpoint saved: model_epoch_1.pt\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mEpoch 1/3: train_loss=0.688897 val_loss=0.6880520383516947 time=0.75s\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.007\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.010\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.012\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.699806\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.159258\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.542391\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.845122\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.062\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.064\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.752532\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.069\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.239814\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.072\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.073\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.549831\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.076\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.818814\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.087\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.131\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.132\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.134\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.139\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.869273\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.142\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.157345\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.143\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.145\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.146\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.541118\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.149\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.846773\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.177\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.182\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.185\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.187\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.731844\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.189\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.191\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.263756\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.192\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.195\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.196\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.471916\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.887300\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.206\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.222\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.233\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.788483\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.236\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.266403\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.239\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.241\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.598883\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.862530\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.279\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.280\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.782142\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.258054\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.527740\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.290\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.291\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.876790\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.315\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0001\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.721995\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.208890\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.327\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.501057\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.335\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.836597\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.341\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.357\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0002\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.365\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.368\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.369\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.791970\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.235808\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.375\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.379\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.552253\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.877462\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.406\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.407\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=-0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.414\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.417\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.837664\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.224304\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.519647\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.428\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.430\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.890262\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.431\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.452\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.859754\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.181943\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.467\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.559722\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.868881\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.480\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.501\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0008\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.507\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.511\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.512\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.740394\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.282301\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.533\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.497006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.876437\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.577\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.583\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.584\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.587\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.880233\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.432224\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.601\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.447925\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.605\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.947446\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([6, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([6])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0008\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.634\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.812619\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.636\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.177778\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.505921\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.763382\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.656\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.740562\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.664\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.310169\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.668\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.434981\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.671\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.673\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.900104\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.677\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.679\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0011\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.572705\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.231808\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.327830\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.699\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.926717\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([12, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([12])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_metrics\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mMetrics (step 2): train_loss=0.680799 epoch=2.000000 epoch_time=0.701288 accuracy=0.565789 val_loss=0.686893\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mNew best model at epoch 2: 0.686893\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_save_checkpoint\u001b[0m:\u001b[36m477\u001b[0m - \u001b[34m\u001b[1mCheckpoint saved: model_epoch_2.pt\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mEpoch 2/3: train_loss=0.680799 val_loss=0.6868934830029806 time=0.70s\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0009\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.732\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.733\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.734\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.756664\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.740\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.364726\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.741\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.572873\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.948632\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.778367\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.787\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.789\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.209061\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.499810\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.803\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.845664\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.852\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.856\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.815629\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.860\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.283386\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.479393\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.872\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.966954\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.903\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.912\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.914\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.837889\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.163283\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.923\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.925\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.554810\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.927\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.929\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.881645\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.931\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.754783\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.967\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.222508\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.555730\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.907772\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:56.999\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.000\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.760154\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.183634\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.018\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.535358\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.876225\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.056\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0003\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.057\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.949180\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.346974\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.071\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.074\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.076\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.545293\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.080\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.922746\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.102\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.107\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.743978\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.117\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.325367\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.118\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.465469\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.131\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.838938\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.133\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.135\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0008\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.198\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.743787\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.245506\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.216\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.218\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.470837\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.911427\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.225\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0011\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.263\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.854337\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.368362\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.280\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.281\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.562284\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.869680\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.290\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0008\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.327\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.820502\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.338541\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.334\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.562340\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.906755\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.342\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.344\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.365\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=0.0013\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.368\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([6, 32]) mean=0.0004\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.369\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.843920\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.340151\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.706606\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([6, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.654096\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([6, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.390\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([6])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0011\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.407\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.410\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.838783\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.254486\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.417\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.525335\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.797080\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.426\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0009\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.435\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.440\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.442\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.771890\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.378309\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.452\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.454\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.457013\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.904511\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0013\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([12, 32]) mean=0.0009\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.483\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.485\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.629565\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.262845\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.497\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.499\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.381287\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.505\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([12, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.507\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.957090\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([12, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.511\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([12])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_metrics\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mMetrics (step 3): train_loss=0.675328 epoch=3.000000 epoch_time=0.792618 accuracy=0.592105 val_loss=0.688819\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_save_checkpoint\u001b[0m:\u001b[36m477\u001b[0m - \u001b[34m\u001b[1mCheckpoint saved: model_epoch_3.pt\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mEpoch 3/3: train_loss=0.675328 val_loss=0.6888193289438883 time=0.79s\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_save_best_model\u001b[0m:\u001b[36m497\u001b[0m - \u001b[1mBest model saved: best_model.pt\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mNexusFormer initialized: 2 encoders, 2 iterations\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m319\u001b[0m - \u001b[1mNexusFlow model artifact saved to: synthetic_demo.nxf\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_create_model_artifact\u001b[0m:\u001b[36m530\u001b[0m - \u001b[1mModel artifact created: synthetic_demo.nxf\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m459\u001b[0m - \u001b[1mTraining complete! Best model at epoch 2 with metric 0.686893\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m538\u001b[0m - \u001b[1mRunning comprehensive evaluation on test set...\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mUsing best model from epoch 2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.578\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0009\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.579\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.584\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.585\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.724034\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.591\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.226846\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.598\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.504060\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.601\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.830754\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0011\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([32, 32]) mean=0.0006\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.612\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.615\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.761082\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.252582\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.625\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.470242\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.627\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([32, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.790610\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.629\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([32, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.633\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([14, 32]) mean=0.0007\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.635\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([14, 32]) mean=0.0005\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.785796\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 2.422084\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/2\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.460694\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([14, 32])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.926940\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([14, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([14])\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_metrics\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mMetrics (step 2): accuracy=0.474359 test_loss=0.711513 num_test_samples=78.000000 num_test_batches=3.000000\u001b[0m\n",
      "\u001b[32m2025-08-25 03:32:57.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m582\u001b[0m - \u001b[1mFinal evaluation metrics: {'accuracy': 0.4743589758872986, 'test_loss': 0.7115134994188944, 'num_test_samples': 78, 'num_test_batches': 3}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n",
      "\n",
      "üìà Evaluating model...\n",
      "\n",
      "üìä Final Evaluation Results:\n",
      "   accuracy: 0.4744\n",
      "   test_loss: 0.7115\n",
      "   num_test_samples: 78.0000\n",
      "   num_test_batches: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Run synthetic training demo\n",
    "synthetic_config, synthetic_trainer = run_synthetic_training_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c880c8",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Section 4: Real Data Preparation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e77476d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample CSV files to demonstrate real data training.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üóÇÔ∏è  SAMPLE DATA CREATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Ensure datasets directory exists\n",
    "    datasets_dir = Path(\"datasets\")\n",
    "    datasets_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    n_customers = 1000\n",
    "    \n",
    "    # 1. Create customers table (demographics + target)\n",
    "    customers_data = {\n",
    "        'customer_id': range(1, n_customers + 1),\n",
    "        'age': np.random.normal(35, 12, n_customers).clip(18, 80).astype(int),\n",
    "        'income': np.random.lognormal(10.5, 0.5, n_customers).clip(20000, 200000).astype(int),\n",
    "        'tenure_months': np.random.exponential(24, n_customers).clip(1, 120).astype(int),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_customers),\n",
    "        'has_churned': np.random.binomial(1, 0.15, n_customers)  # 15% churn rate\n",
    "    }\n",
    "    \n",
    "    customers_df = pd.DataFrame(customers_data)\n",
    "    customers_df.to_csv(\"datasets/customers.csv\", index=False)\n",
    "    print(f\"‚úÖ Created customers.csv: {customers_df.shape}\")\n",
    "    \n",
    "    # 2. Create transactions table (behavioral data)\n",
    "    # Generate multiple transactions per customer\n",
    "    transactions_data = []\n",
    "    for customer_id in range(1, n_customers + 1):\n",
    "        n_transactions = np.random.poisson(8)  # Average 8 transactions per customer\n",
    "        for _ in range(n_transactions):\n",
    "            transactions_data.append({\n",
    "                'customer_id': customer_id,\n",
    "                'transaction_amount': np.clip(np.random.lognormal(3, 1), 5, 1000),\n",
    "                'days_since_last': np.clip(np.random.exponential(15), 1, 90),\n",
    "                'transaction_type': np.random.choice(['online', 'store', 'phone'], p=[0.6, 0.3, 0.1])\n",
    "            })\n",
    "    \n",
    "    transactions_df = pd.DataFrame(transactions_data)\n",
    "    \n",
    "    # Aggregate transactions by customer for this demo\n",
    "    agg_transactions = transactions_df.groupby('customer_id').agg({\n",
    "        'transaction_amount': ['mean', 'std', 'count'],\n",
    "        'days_since_last': ['mean', 'min']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    agg_transactions.columns = ['_'.join(col).strip() for col in agg_transactions.columns]\n",
    "    agg_transactions = agg_transactions.reset_index()\n",
    "    \n",
    "    # Add transaction type preferences\n",
    "    type_counts = transactions_df.groupby('customer_id')['transaction_type'].value_counts().unstack(fill_value=0)\n",
    "    type_proportions = type_counts.div(type_counts.sum(axis=1), axis=0).round(3)\n",
    "    type_proportions = type_proportions.reset_index()\n",
    "    \n",
    "    transactions_final = agg_transactions.merge(type_proportions, on='customer_id')\n",
    "    transactions_final.to_csv(\"datasets/transactions.csv\", index=False)\n",
    "    print(f\"‚úÖ Created transactions.csv: {transactions_final.shape}\")\n",
    "    \n",
    "    # 3. Create support tickets table (text-like features)\n",
    "    support_data = []\n",
    "    for customer_id in range(1, n_customers + 1):\n",
    "        if np.random.random() < 0.3:  # 30% of customers have support tickets\n",
    "            n_tickets = np.random.poisson(2) + 1\n",
    "            for _ in range(n_tickets):\n",
    "                support_data.append({\n",
    "                    'customer_id': customer_id,\n",
    "                    'ticket_priority': np.random.choice(['low', 'medium', 'high'], p=[0.5, 0.4, 0.1]),\n",
    "                    'resolution_days': np.clip(np.random.exponential(3), 1, 30),\n",
    "                    'satisfaction_score': np.clip(np.random.normal(3.5, 1), 1, 5)\n",
    "                })\n",
    "    \n",
    "    if support_data:\n",
    "        support_df = pd.DataFrame(support_data)\n",
    "        \n",
    "        # Aggregate support tickets by customer\n",
    "        agg_support = support_df.groupby('customer_id').agg({\n",
    "            'resolution_days': ['mean', 'max'],\n",
    "            'satisfaction_score': ['mean', 'min']\n",
    "        }).round(2)\n",
    "        \n",
    "        agg_support.columns = ['_'.join(col).strip() for col in agg_support.columns]\n",
    "        agg_support = agg_support.reset_index()\n",
    "        \n",
    "        # Add priority distributions\n",
    "        priority_counts = support_df.groupby('customer_id')['ticket_priority'].value_counts().unstack(fill_value=0)\n",
    "        priority_props = priority_counts.div(priority_counts.sum(axis=1), axis=0).round(3)\n",
    "        priority_props.columns = [f'priority_{col}_pct' for col in priority_props.columns]\n",
    "        priority_props = priority_props.reset_index()\n",
    "        \n",
    "        support_final = agg_support.merge(priority_props, on='customer_id', how='left').fillna(0)\n",
    "        support_final.to_csv(\"datasets/support.csv\", index=False)\n",
    "        print(f\"‚úÖ Created support.csv: {support_final.shape}\")\n",
    "    \n",
    "    print(f\"\\nüìã Dataset Summary:\")\n",
    "    print(f\"   Customers: {len(customers_df)} records\")\n",
    "    print(f\"   Transactions: {len(transactions_final)} aggregated records\") \n",
    "    print(f\"   Support: {len(support_final) if 'support_final' in locals() else 0} records\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef2850a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_real_data_training():\n",
    "    \"\"\"Demonstrate training with real CSV data.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà REAL DATA TRAINING DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First create sample data\n",
    "    create_sample_datasets()\n",
    "    \n",
    "    # Create configuration for real data\n",
    "    real_data_config = {\n",
    "        'project_name': 'churn_prediction',\n",
    "        'primary_key': 'customer_id',\n",
    "        'target': {\n",
    "            'target_table': 'customers.csv',\n",
    "            'target_column': 'has_churned'\n",
    "        },\n",
    "        'architecture': {\n",
    "            'refinement_iterations': 3,\n",
    "            'global_embed_dim': 64\n",
    "        },\n",
    "        'datasets': [\n",
    "            {'name': 'customers.csv', 'transformer_type': 'standard', 'complexity': 'medium'},\n",
    "            {'name': 'transactions.csv', 'transformer_type': 'standard', 'complexity': 'medium'},\n",
    "            {'name': 'support.csv', 'transformer_type': 'standard', 'complexity': 'small'}\n",
    "        ],\n",
    "        'training': {\n",
    "            'use_synthetic': False,  # Use real data\n",
    "            'batch_size': 64,\n",
    "            'epochs': 5,\n",
    "            'optimizer': {'name': 'adam', 'lr': 5e-4},\n",
    "            'split_config': {\n",
    "                'test_size': 0.15,\n",
    "                'validation_size': 0.15,\n",
    "                'randomize': True\n",
    "            }\n",
    "        },\n",
    "        'mlops': {\n",
    "            'logging_provider': 'stdout',\n",
    "            'experiment_name': 'churn_prediction_v1'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config = ConfigModel.model_validate(real_data_config)\n",
    "    print(\"‚úÖ Configuration created for real data training\")\n",
    "    \n",
    "    # Initialize and train\n",
    "    try:\n",
    "        trainer = Trainer(config, work_dir=\".\")\n",
    "        print(\"‚úÖ Trainer initialized with real data\")\n",
    "        \n",
    "        # Show data summary\n",
    "        print(\"\\nüìä Data Summary:\")\n",
    "        if trainer.datasets:\n",
    "            for name, df in trainer.datasets.items():\n",
    "                missing_pct = df.isnull().sum().sum() / df.size * 100\n",
    "                print(f\"   {name}: {df.shape} ({missing_pct:.1f}% missing)\")\n",
    "        print(f\"   Input dimensions: {trainer.input_dims}\")\n",
    "        \n",
    "        # Run sanity check\n",
    "        print(\"\\nüîç Running comprehensive data validation...\")\n",
    "        trainer.sanity_check()\n",
    "        print(\"‚úÖ Data validation passed\")\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nüöÄ Training on real data...\")\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ Real data training completed!\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\nüìà Final evaluation...\")\n",
    "        metrics = trainer.evaluate()\n",
    "        \n",
    "        if metrics:\n",
    "            print(\"\\nüéØ Model Performance:\")\n",
    "            for metric, value in metrics.items():\n",
    "                if 'accuracy' in metric:\n",
    "                    print(f\"   {metric}: {value:.2%}\")\n",
    "                else:\n",
    "                    print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        return config, trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Real data training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af5f6728",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà REAL DATA TRAINING DEMO\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üóÇÔ∏è  SAMPLE DATA CREATION\n",
      "============================================================\n",
      "‚úÖ Created customers.csv: (1000, 6)\n",
      "‚úÖ Created transactions.csv: (1000, 9)\n",
      "‚úÖ Created support.csv: (307, 8)\n",
      "\n",
      "üìã Dataset Summary:\n",
      "   Customers: 1000 records\n",
      "   Transactions: 1000 aggregated records\n",
      "   Support: 307 records\n",
      "‚úÖ Configuration created for real data training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-25 19:43:38.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_file_logging\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mFile logging enabled: results\\logs\\training_churn_prediction_v1.log\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mEnhanced Trainer initialized (device=cpu)\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m190\u001b[0m - \u001b[1mLoading and aligning real datasets...\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoaded table: datasets/customers.csv rows=1000 cols=6\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mMissing values: total=0 (0.0%)\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mNo missing values found\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mDataset customers.csv: shape=(1000, 6), missing_values=0\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoaded table: datasets/transactions.csv rows=1000 cols=9\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mMissing values: total=1 (0.0%)\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mMissing by column: {'transaction_amount_std': 1}\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mDataset transactions.csv: shape=(1000, 9), missing_values=1\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m70\u001b[0m - \u001b[34m\u001b[1mMissing values by column in transactions.csv: {'customer_id': 0, 'transaction_amount_mean': 0, 'transaction_amount_std': 1, 'transaction_amount_count': 0, 'days_since_last_mean': 0, 'days_since_last_min': 0, 'online': 0, 'phone': 0, 'store': 0}\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoaded table: datasets/support.csv rows=307 cols=8\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mMissing values: total=0 (0.0%)\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.708\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_table\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mNo missing values found\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mload_datasets\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mDataset support.csv: shape=(307, 8), missing_values=0\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.713\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mDataset customers.csv: 1000 unique keys\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mDataset transactions.csv: 1000 unique keys\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mDataset support.csv: 307 unique keys\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mFound 307 common keys across all datasets\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1mAligned customers.csv: 307 rows\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1mAligned transactions.csv: 307 rows\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.729\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36malign_datasets\u001b[0m:\u001b[36m111\u001b[0m - \u001b[34m\u001b[1mAligned support.csv: 307 rows\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset customers.csv: 4 features from columns ['age', 'income', 'tenure_months', 'region']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.733\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset transactions.csv: 8 features from columns ['transaction_amount_mean', 'transaction_amount_std', 'transaction_amount_count', 'days_since_last_mean', 'days_since_last_min', 'online', 'phone', 'store']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.734\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset support.csv: 7 features from columns ['resolution_days_mean', 'resolution_days_max', 'satisfaction_score_mean', 'satisfaction_score_min', 'priority_high_pct', 'priority_low_pct', 'priority_medium_pct']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mData alignment complete:\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1m  Aligned samples: 307\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1m  Datasets: 3\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m  Feature dimensions: [4, 8, 7]\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1m  Total features: 19\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m212\u001b[0m - \u001b[34m\u001b[1m  customers.csv: 0.00% missing values\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.745\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m212\u001b[0m - \u001b[34m\u001b[1m  transactions.csv: 0.00% missing values\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m_setup_data\u001b[0m:\u001b[36m212\u001b[0m - \u001b[34m\u001b[1m  support.csv: 0.00% missing values\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:38.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mNexusFormer initialized: 3 encoders, 3 iterations\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36mlog_params\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLogging parameters: {'model_name': 'NexusFormer', 'input_dims': [4, 8, 7], 'embed_dim': 64, 'refinement_iterations': 3, 'num_parameters': 267585, 'learning_rate': 0.0005, 'optimizer': 'adam', 'batch_size': 64, 'epochs': 5}\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mModel initialized: 267585 parameters\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mRunning comprehensive sanity checks...\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([2, 64]) mean=-0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.842\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 0: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([2, 64]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 1: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.849\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mStandardTabularEncoder output: shape=torch.Size([2, 64]) mean=0.0000\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mInitial encoding 2: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.852\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 1/3\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.868\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.970033\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.951020\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 2 attention change: 1.902286\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 2/3\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.833687\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.884\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.893737\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 2 attention change: 1.758716\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mRefinement iteration 3/3\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 0 attention change: 1.605164\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.894\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 1 attention change: 1.708205\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mCrossContextualAttention output: shape=torch.Size([2, 64])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.899\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mEncoder 2 attention change: 1.628226\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mFinal concatenated shape: torch.Size([2, 192])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mNexusFormer final output: shape=torch.Size([2])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.nexusflow.trainer.trainer\u001b[0m:\u001b[36msanity_check\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mModel forward pass: output_shape=torch.Size([2])\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.912\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36msplit_df\u001b[0m:\u001b[36m135\u001b[0m - \u001b[34m\u001b[1mSplit sizes -> train=214 val=46 test=47\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset customers.csv: 4 features from columns ['age', 'income', 'tenure_months', 'region']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset transactions.csv: 8 features from columns ['transaction_amount_mean', 'transaction_amount_std', 'transaction_amount_count', 'days_since_last_mean', 'days_since_last_min', 'online', 'phone', 'store']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset support.csv: 7 features from columns ['resolution_days_mean', 'resolution_days_max', 'satisfaction_score_mean', 'satisfaction_score_min', 'priority_high_pct', 'priority_low_pct', 'priority_medium_pct']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset customers.csv: 4 features from columns ['age', 'income', 'tenure_months', 'region']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.926\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset transactions.csv: 8 features from columns ['transaction_amount_mean', 'transaction_amount_std', 'transaction_amount_count', 'days_since_last_mean', 'days_since_last_min', 'online', 'phone', 'store']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.928\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset support.csv: 7 features from columns ['resolution_days_mean', 'resolution_days_max', 'satisfaction_score_mean', 'satisfaction_score_min', 'priority_high_pct', 'priority_low_pct', 'priority_medium_pct']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset customers.csv: 4 features from columns ['age', 'income', 'tenure_months', 'region']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset transactions.csv: 8 features from columns ['transaction_amount_mean', 'transaction_amount_std', 'transaction_amount_count', 'days_since_last_mean', 'days_since_last_min', 'online', 'phone', 'store']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mget_feature_dimensions\u001b[0m:\u001b[36m162\u001b[0m - \u001b[34m\u001b[1mDataset support.csv: 7 features from columns ['resolution_days_mean', 'resolution_days_max', 'satisfaction_score_mean', 'satisfaction_score_min', 'priority_high_pct', 'priority_low_pct', 'priority_medium_pct']\u001b[0m\n",
      "\u001b[32m2025-08-25 19:43:52.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.data.ingestion\u001b[0m:\u001b[36mmake_dataloaders\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mCreated DataLoaders: train=4 val=1 test=1 batches\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arkch\\AppData\\Local\\Temp\\ipykernel_36208\\3067339289.py\", line 62, in demo_real_data_training\n",
      "    trainer.sanity_check()\n",
      "  File \"c:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\src\\nexusflow\\trainer\\trainer.py\", line 359, in sanity_check\n",
      "    sample_batch = next(iter(self.train_loader))\n",
      "  File \"C:\\Users\\arkch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\arkch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"C:\\Users\\arkch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\arkch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"c:\\Users\\arkch\\OneDrive\\Documents\\VS Code Workspace\\python_files\\ML_projects\\Evoformer\\src\\nexusflow\\data\\dataset.py\", line 49, in __getitem__\n",
      "    features = torch.tensor(row[self.feature_cols].values.astype('float32'))\n",
      "ValueError: could not convert string to float: 'South'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized with real data\n",
      "\n",
      "üìä Data Summary:\n",
      "   customers.csv: (307, 6) (0.0% missing)\n",
      "   transactions.csv: (307, 9) (0.0% missing)\n",
      "   support.csv: (307, 8) (0.0% missing)\n",
      "   Input dimensions: [4, 8, 7]\n",
      "\n",
      "üîç Running comprehensive data validation...\n",
      "‚ùå Real data training failed: could not convert string to float: 'South'\n"
     ]
    }
   ],
   "source": [
    "# Run real data training demo\n",
    "real_config, real_trainer = demo_real_data_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921e537",
   "metadata": {},
   "source": [
    "# üîç Section 5: Model Inspection & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83042b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_model_inference():\n",
    "    \"\"\"Show how to load and use trained models for inference.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç MODEL INFERENCE & INSPECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Look for trained model files\n",
    "    model_files = list(Path(\".\").glob(\"*.nxf\"))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"‚ö†Ô∏è  No .nxf model files found. Train a model first.\")\n",
    "        return\n",
    "    \n",
    "    # Use the most recent model file\n",
    "    latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"üìÅ Loading model: {latest_model}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model artifact\n",
    "        model = load_model(str(latest_model))\n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "        # Show model summary\n",
    "        print(\"\\nüìã Model Summary:\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        # Show model parameters\n",
    "        print(\"\\n‚öôÔ∏è  Model Parameters:\")\n",
    "        params = model.get_params()\n",
    "        key_params = ['total_parameters', 'input_dimensions', 'architecture', 'training_info']\n",
    "        for key in key_params:\n",
    "            if key in params:\n",
    "                print(f\"   {key}: {params[key]}\")\n",
    "        \n",
    "        # Demonstrate inference with sample data\n",
    "        print(\"\\nüîÆ Sample Inference:\")\n",
    "        \n",
    "        # Create sample input data matching the model's expected format\n",
    "        input_dims = params['input_dimensions']\n",
    "        batch_size = 5\n",
    "        \n",
    "        # Generate random sample data\n",
    "        sample_data = []\n",
    "        for dim in input_dims:\n",
    "            sample_table = pd.DataFrame(\n",
    "                np.random.randn(batch_size, dim),\n",
    "                columns=[f'feature_{i}' for i in range(dim)]\n",
    "            )\n",
    "            sample_data.append(sample_table)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(sample_data)\n",
    "        print(f\"   Input: {len(sample_data)} tables with dims {input_dims}\")\n",
    "        print(f\"   Output: {predictions.shape} predictions\")\n",
    "        print(f\"   Sample predictions: {predictions[:3].round(4)}\")\n",
    "        \n",
    "        # Try the visualization (placeholder)\n",
    "        print(\"\\nüé® Model Visualization:\")\n",
    "        viz_info = model.visualize_flow()\n",
    "        if viz_info:\n",
    "            print(f\"   Architecture: {viz_info.get('architecture')}\")\n",
    "            print(f\"   Encoders: {viz_info.get('num_encoders')}\")\n",
    "            print(f\"   Refinement iterations: {viz_info.get('refinement_iterations')}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading/inference failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2e1f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run inference demo\n",
    "loaded_model = demonstrate_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f64b0",
   "metadata": {},
   "source": [
    "# üìä Section 6: Advanced Features & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14365abd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_training_results():\n",
    "    \"\"\"Analyze and visualize training results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä TRAINING RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Look for results and logs\n",
    "    results_dir = Path(\"results\")\n",
    "    if not results_dir.exists():\n",
    "        print(\"‚ö†Ô∏è  No results directory found\")\n",
    "        return\n",
    "    \n",
    "    # Check for metrics files\n",
    "    metrics_file = results_dir / \"metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        import json\n",
    "        with open(metrics_file) as f:\n",
    "            metrics_data = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Found training metrics: {len(metrics_data)} logged steps\")\n",
    "        \n",
    "        # Extract metrics for plotting\n",
    "        steps = []\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for entry in metrics_data:\n",
    "            if 'metrics' in entry and entry['step']:\n",
    "                steps.append(entry['step'])\n",
    "                train_losses.append(entry['metrics'].get('train_loss'))\n",
    "                val_losses.append(entry['metrics'].get('val_loss'))\n",
    "        \n",
    "        # Create training curve plot\n",
    "        if steps and train_losses:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(steps, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "            if any(v is not None for v in val_losses):\n",
    "                val_losses_clean = [v for v in val_losses if v is not None]\n",
    "                val_steps = [s for s, v in zip(steps, val_losses) if v is not None]\n",
    "                plt.plot(val_steps, val_losses_clean, 'r-', label='Validation Loss', linewidth=2)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Curves')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Model architecture visualization\n",
    "            plt.subplot(1, 2, 2)\n",
    "            if loaded_model:\n",
    "                params = loaded_model.get_params()\n",
    "                input_dims = params.get('input_dimensions', [])\n",
    "                \n",
    "                # Create a simple bar chart of input dimensions\n",
    "                if input_dims:\n",
    "                    plt.bar(range(len(input_dims)), input_dims, \n",
    "                           color=sns.color_palette(\"husl\", len(input_dims)))\n",
    "                    plt.xlabel('Encoder Index')\n",
    "                    plt.ylabel('Input Features')\n",
    "                    plt.title('Model Input Dimensions')\n",
    "                    for i, dim in enumerate(input_dims):\n",
    "                        plt.text(i, dim + max(input_dims)*0.01, str(dim), \n",
    "                               ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = list(Path(\".\").glob(\"model_epoch_*.pt\"))\n",
    "    if checkpoint_files:\n",
    "        print(f\"\\nüìÅ Found {len(checkpoint_files)} training checkpoints\")\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"   Latest: {latest_checkpoint.name}\")\n",
    "    \n",
    "    # Check for best model\n",
    "    best_model = Path(\"best_model.pt\")\n",
    "    if best_model.exists():\n",
    "        print(f\"‚úÖ Best model checkpoint available: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caa05e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_config_variations():\n",
    "    \"\"\"Show different configuration options and their effects.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚öôÔ∏è  CONFIGURATION VARIATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    configs = {\n",
    "        \"Minimal Setup\": {\n",
    "            'project_name': 'minimal_test',\n",
    "            'primary_key': 'id',\n",
    "            'target': {'target_table': 'data.csv', 'target_column': 'label'},\n",
    "            'datasets': [{'name': 'data.csv'}],\n",
    "            'architecture': {'global_embed_dim': 32},\n",
    "            'training': {'use_synthetic': True, 'epochs': 2}\n",
    "        },\n",
    "        \n",
    "        \"Complex Multi-Table\": {\n",
    "            'project_name': 'complex_system',\n",
    "            'primary_key': 'user_id', \n",
    "            'target': {'target_table': 'users.csv', 'target_column': 'conversion'},\n",
    "            'datasets': [\n",
    "                {'name': 'users.csv', 'transformer_type': 'standard', 'complexity': 'large'},\n",
    "                {'name': 'events.csv', 'transformer_type': 'timeseries', 'complexity': 'medium'},\n",
    "                {'name': 'content.csv', 'transformer_type': 'text', 'complexity': 'medium'}\n",
    "            ],\n",
    "            'architecture': {'global_embed_dim': 128, 'refinement_iterations': 5},\n",
    "            'training': {'batch_size': 128, 'epochs': 50, 'optimizer': {'name': 'adam', 'lr': 1e-4}}\n",
    "        },\n",
    "        \n",
    "        \"High Performance\": {\n",
    "            'project_name': 'high_performance',\n",
    "            'primary_key': 'sample_id',\n",
    "            'target': {'target_table': 'outcomes.csv', 'target_column': 'target'},\n",
    "            'datasets': [\n",
    "                {'name': f'features_{i}.csv', 'complexity': 'medium'} for i in range(5)\n",
    "            ],\n",
    "            'architecture': {'global_embed_dim': 256, 'refinement_iterations': 8},\n",
    "            'training': {\n",
    "                'batch_size': 256, \n",
    "                'epochs': 100,\n",
    "                'optimizer': {'name': 'adam', 'lr': 2e-4},\n",
    "                'split_config': {'test_size': 0.1, 'validation_size': 0.1}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\nüìã {name}:\")\n",
    "        print(f\"   Datasets: {len(config['datasets'])}\")\n",
    "        print(f\"   Embedding dim: {config.get('architecture', {}).get('global_embed_dim', 64)}\")\n",
    "        print(f\"   Epochs: {config.get('training', {}).get('epochs', 10)}\")\n",
    "        if 'refinement_iterations' in config.get('architecture', {}):\n",
    "            print(f\"   Refinement loops: {config['architecture']['refinement_iterations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37039214",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run analysis functions\n",
    "analyze_training_results()\n",
    "demonstrate_config_variations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab021566",
   "metadata": {},
   "source": [
    "# üéØ Section 7: Production Usage Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de33d04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def show_production_patterns():\n",
    "    \"\"\"Demonstrate production usage patterns and best practices.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ PRODUCTION USAGE PATTERNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"üîß 1. Batch Inference Pipeline:\")\n",
    "    print(\"\"\"\n",
    "    # Load trained model\n",
    "    model = nexusflow.load_model('models/production_model.nxf')\n",
    "    \n",
    "    # Process new data in batches\n",
    "    batch_size = 1000\n",
    "    predictions = []\n",
    "    \n",
    "    for batch_data in data_loader:\n",
    "        batch_pred = model.predict(batch_data)\n",
    "        predictions.extend(batch_pred)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüîß 2. API Endpoint Integration:\")\n",
    "    print(\"\"\"\n",
    "    from flask import Flask, request, jsonify\n",
    "    import nexusflow\n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    model = nexusflow.load_model('production_model.nxf')\n",
    "    \n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict():\n",
    "        data = request.json\n",
    "        # Convert JSON to required DataFrame format\n",
    "        predictions = model.predict(data)\n",
    "        return jsonify({'predictions': predictions.tolist()})\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüîß 3. Model Monitoring & Updates:\")\n",
    "    print(\"\"\"\n",
    "    # Performance monitoring\n",
    "    def monitor_model_drift(model, new_data, reference_stats):\n",
    "        current_stats = compute_data_statistics(new_data)\n",
    "        drift_score = calculate_drift(current_stats, reference_stats)\n",
    "        \n",
    "        if drift_score > DRIFT_THRESHOLD:\n",
    "            trigger_retraining_pipeline()\n",
    "    \n",
    "    # A/B testing framework\n",
    "    def ab_test_models(model_a, model_b, test_data):\n",
    "        predictions_a = model_a.predict(test_data)\n",
    "        predictions_b = model_b.predict(test_data)\n",
    "        \n",
    "        # Compare performance metrics\n",
    "        return compare_performance(predictions_a, predictions_b)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüîß 4. MLOps Best Practices:\")\n",
    "    best_practices = [\n",
    "        \"Version control your config.yaml files\",\n",
    "        \"Use experiment tracking (W&B, MLflow)\",\n",
    "        \"Implement automated model validation\",\n",
    "        \"Set up continuous integration for model training\",\n",
    "        \"Monitor feature distributions in production\",\n",
    "        \"Implement model rollback mechanisms\",\n",
    "        \"Use feature stores for consistent data access\",\n",
    "        \"Implement proper error handling and logging\"\n",
    "    ]\n",
    "    \n",
    "    for i, practice in enumerate(best_practices, 1):\n",
    "        print(f\"   {i}. {practice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034780e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_deployment_checklist():\n",
    "    \"\"\"Create a deployment readiness checklist.\"\"\"\n",
    "    print(\"\\nüìã DEPLOYMENT READINESS CHECKLIST:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    checklist_items = [\n",
    "        (\"‚úÖ\", \"Model trained and validated on representative data\"),\n",
    "        (\"‚úÖ\", \"Configuration files version controlled\"),\n",
    "        (\"‚úÖ\", \"Performance benchmarks established\"),\n",
    "        (\"‚ö†Ô∏è\", \"Data drift monitoring implemented\"),\n",
    "        (\"‚ö†Ô∏è\", \"A/B testing framework ready\"),\n",
    "        (\"‚ö†Ô∏è\", \"Model rollback procedure tested\"),\n",
    "        (\"‚ùå\", \"Production API endpoints implemented\"),\n",
    "        (\"‚ùå\", \"Monitoring and alerting configured\"),\n",
    "        (\"‚ùå\", \"Documentation and runbooks created\"),\n",
    "        (\"‚ùå\", \"Security review completed\")\n",
    "    ]\n",
    "    \n",
    "    for status, item in checklist_items:\n",
    "        print(f\"{status} {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d56c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "show_production_patterns()\n",
    "create_deployment_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b6555",
   "metadata": {},
   "source": [
    "# üéì Section 8: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d827876",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_summary():\n",
    "    \"\"\"Create a comprehensive summary of what we've covered.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéì TUTORIAL SUMMARY & NEXT STEPS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"üèÜ What You've Learned:\")\n",
    "    achievements = [\n",
    "        \"Project initialization and structure\",\n",
    "        \"Configuration management with YAML\",\n",
    "        \"Training with both synthetic and real data\",\n",
    "        \"Multi-table data alignment and processing\", \n",
    "        \"Model evaluation and performance analysis\",\n",
    "        \"Model artifact creation and inference\",\n",
    "        \"Production deployment considerations\"\n",
    "    ]\n",
    "    \n",
    "    for achievement in achievements:\n",
    "        print(f\"   ‚úÖ {achievement}\")\n",
    "    \n",
    "    print(\"\\nüöÄ Next Steps to Explore:\")\n",
    "    next_steps = [\n",
    "        \"Experiment with different architectures (embed_dim, refinement_iterations)\",\n",
    "        \"Try heterogeneous data types (text, timeseries)\",\n",
    "        \"Implement custom encoders for domain-specific data\",\n",
    "        \"Set up MLOps integration (W&B, MLflow)\",\n",
    "        \"Build production API endpoints\",\n",
    "        \"Implement model monitoring and drift detection\",\n",
    "        \"Explore the visualization capabilities\",\n",
    "        \"Contribute to the NexusFlow project\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(next_steps, 1):\n",
    "        print(f\"   {i}. {step}\")\n",
    "    \n",
    "    print(\"\\nüìö Key Resources:\")\n",
    "    resources = [\n",
    "        \"NexusFlow Documentation: Check project README\",\n",
    "        \"Configuration Examples: See configs/ directory\", \n",
    "        \"API Reference: nexusflow.api.model_api module\",\n",
    "        \"Model Architecture: nexusflow.model.nexus_former module\",\n",
    "        \"Training Pipeline: nexusflow.trainer.trainer module\"\n",
    "    ]\n",
    "    \n",
    "    for resource in resources:\n",
    "        print(f\"   üìñ {resource}\")\n",
    "    \n",
    "    print(\"\\nüí° Pro Tips:\")\n",
    "    tips = [\n",
    "        \"Start with synthetic data to validate your setup\",\n",
    "        \"Use smaller embed_dim for faster prototyping\",\n",
    "        \"Monitor validation loss to prevent overfitting\", \n",
    "        \"Experiment with different refinement_iterations\",\n",
    "        \"Save your successful configurations for reuse\",\n",
    "        \"Use the CLI for production training workflows\"\n",
    "    ]\n",
    "    \n",
    "    for tip in tips:\n",
    "        print(f\"   üí° {tip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ab0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511654d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Tutorial Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Thank you for exploring NexusFlow!\")\n",
    "print(\"üîó Ready to build your own multi-transformer tabular ecosystems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b346b6",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Section 9: Interactive Utilities & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884f1e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_interactive_model_explorer():\n",
    "    \"\"\"Create an interactive model exploration widget.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üõ†Ô∏è  INTERACTIVE MODEL EXPLORER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def explore_model_architecture():\n",
    "        \"\"\"Interactive function to explore different architectures.\"\"\"\n",
    "        print(\"üèóÔ∏è  Architecture Explorer\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Interactive parameter selection\n",
    "        embed_dims = [32, 64, 128, 256]\n",
    "        refinement_iterations = [1, 2, 3, 5, 8]\n",
    "        \n",
    "        print(\"Available embedding dimensions:\", embed_dims)\n",
    "        print(\"Available refinement iterations:\", refinement_iterations)\n",
    "        \n",
    "        # Calculate parameter counts for different configurations\n",
    "        print(\"\\nüìä Parameter Count Estimates:\")\n",
    "        for embed_dim in [64, 128]:\n",
    "            for ref_iter in [2, 5]:\n",
    "                # Rough parameter estimate (simplified)\n",
    "                encoder_params = embed_dim * 50  # Rough estimate per encoder\n",
    "                attention_params = embed_dim * embed_dim * 4  # Q,K,V,O projections\n",
    "                fusion_params = embed_dim * 3 * 32  # Fusion layers\n",
    "                \n",
    "                total_params = 2 * encoder_params + 2 * attention_params + fusion_params\n",
    "                print(f\"   embed_dim={embed_dim}, iterations={ref_iter}: ~{total_params:,} parameters\")\n",
    "        \n",
    "        return embed_dims, refinement_iterations\n",
    "    \n",
    "    def model_performance_predictor():\n",
    "        \"\"\"Predict approximate training time and memory usage.\"\"\"\n",
    "        print(\"\\n‚è±Ô∏è  Performance Predictor\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        configs = [\n",
    "            {\"name\": \"Small\", \"embed_dim\": 32, \"iterations\": 2, \"datasets\": 2},\n",
    "            {\"name\": \"Medium\", \"embed_dim\": 64, \"iterations\": 3, \"datasets\": 3}, \n",
    "            {\"name\": \"Large\", \"embed_dim\": 128, \"iterations\": 5, \"datasets\": 4},\n",
    "            {\"name\": \"XLarge\", \"embed_dim\": 256, \"iterations\": 8, \"datasets\": 5}\n",
    "        ]\n",
    "        \n",
    "        for config in configs:\n",
    "            # Rough estimates based on typical hardware\n",
    "            train_time_per_epoch = config[\"embed_dim\"] * config[\"iterations\"] * config[\"datasets\"] * 0.001\n",
    "            memory_mb = config[\"embed_dim\"] * config[\"datasets\"] * 0.5\n",
    "            \n",
    "            print(f\"   {config['name']:>7}: ~{train_time_per_epoch:.1f}s/epoch, ~{memory_mb:.0f}MB memory\")\n",
    "    \n",
    "    explore_model_architecture()\n",
    "    model_performance_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f182f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_data_quality_analyzer():\n",
    "    \"\"\"Create tools for analyzing data quality and preparation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç DATA QUALITY ANALYZER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def analyze_dataset_compatibility():\n",
    "        \"\"\"Check if datasets are compatible for NexusFlow training.\"\"\"\n",
    "        print(\"üîó Dataset Compatibility Checker\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Check if our sample datasets exist\n",
    "        dataset_files = [\"customers.csv\", \"transactions.csv\", \"support.csv\"]\n",
    "        datasets_dir = Path(\"datasets\")\n",
    "        \n",
    "        compatibility_report = {}\n",
    "        \n",
    "        for filename in dataset_files:\n",
    "            filepath = datasets_dir / filename\n",
    "            if filepath.exists():\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    \n",
    "                    # Analyze dataset characteristics\n",
    "                    analysis = {\n",
    "                        \"shape\": df.shape,\n",
    "                        \"missing_values\": df.isnull().sum().sum(),\n",
    "                        \"missing_percentage\": (df.isnull().sum().sum() / df.size) * 100,\n",
    "                        \"numeric_columns\": len(df.select_dtypes(include=[np.number]).columns),\n",
    "                        \"categorical_columns\": len(df.select_dtypes(include=['object']).columns),\n",
    "                        \"has_customer_id\": 'customer_id' in df.columns,\n",
    "                        \"duplicate_ids\": df['customer_id'].duplicated().sum() if 'customer_id' in df.columns else 0\n",
    "                    }\n",
    "                    \n",
    "                    compatibility_report[filename] = analysis\n",
    "                    \n",
    "                    # Print summary\n",
    "                    status = \"‚úÖ\" if analysis[\"missing_percentage\"] < 10 and analysis[\"has_customer_id\"] else \"‚ö†Ô∏è\"\n",
    "                    print(f\"   {status} {filename}: {df.shape[0]:,} rows √ó {df.shape[1]} cols, {analysis['missing_percentage']:.1f}% missing\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå {filename}: Error reading file - {e}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå {filename}: File not found\")\n",
    "        \n",
    "        return compatibility_report\n",
    "    \n",
    "    def suggest_data_improvements():\n",
    "        \"\"\"Suggest improvements for better model performance.\"\"\"\n",
    "        print(\"\\nüí° Data Improvement Suggestions\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        suggestions = [\n",
    "            \"üîπ Feature Engineering: Create interaction features between tables\",\n",
    "            \"üîπ Temporal Features: Add time-based aggregations (recent vs. historical)\",\n",
    "            \"üîπ Categorical Encoding: Use target encoding for high-cardinality categories\",\n",
    "            \"üîπ Missing Value Strategy: Consider domain-specific imputation methods\",\n",
    "            \"üîπ Feature Scaling: Normalize numerical features within each table\",\n",
    "            \"üîπ Outlier Treatment: Cap extreme values or use robust transformations\",\n",
    "            \"üîπ Data Validation: Implement schema validation for production data\",\n",
    "            \"üîπ Feature Selection: Remove highly correlated or low-variance features\"\n",
    "        ]\n",
    "        \n",
    "        for suggestion in suggestions:\n",
    "            print(f\"   {suggestion}\")\n",
    "    \n",
    "    report = analyze_dataset_compatibility()\n",
    "    suggest_data_improvements()\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b5fc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_experiment_tracker():\n",
    "    \"\"\"Create a simple experiment tracking utility.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ EXPERIMENT TRACKER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiments_log = []\n",
    "    \n",
    "    def log_experiment(name, config, results=None):\n",
    "        \"\"\"Log an experiment configuration and results.\"\"\"\n",
    "        experiment = {\n",
    "            \"name\": name,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"config\": config,\n",
    "            \"results\": results or {}\n",
    "        }\n",
    "        experiments_log.append(experiment)\n",
    "        \n",
    "        print(f\"üìù Logged experiment: {name}\")\n",
    "        return len(experiments_log) - 1  # Return experiment ID\n",
    "    \n",
    "    def compare_experiments(exp_ids=None):\n",
    "        \"\"\"Compare multiple experiments.\"\"\"\n",
    "        if not experiments_log:\n",
    "            print(\"No experiments logged yet\")\n",
    "            return\n",
    "        \n",
    "        if exp_ids is None:\n",
    "            exp_ids = list(range(len(experiments_log)))\n",
    "        \n",
    "        print(f\"üîç Comparing {len(exp_ids)} experiments:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for exp_id in exp_ids:\n",
    "            if exp_id < len(experiments_log):\n",
    "                exp = experiments_log[exp_id]\n",
    "                \n",
    "                # Extract key metrics\n",
    "                config = exp.get(\"config\", {})\n",
    "                results = exp.get(\"results\", {})\n",
    "                \n",
    "                row = {\n",
    "                    \"Name\": exp[\"name\"],\n",
    "                    \"Embed_Dim\": config.get(\"architecture\", {}).get(\"global_embed_dim\", \"N/A\"),\n",
    "                    \"Refinements\": config.get(\"architecture\", {}).get(\"refinement_iterations\", \"N/A\"),\n",
    "                    \"Epochs\": config.get(\"training\", {}).get(\"epochs\", \"N/A\"),\n",
    "                    \"Batch_Size\": config.get(\"training\", {}).get(\"batch_size\", \"N/A\"),\n",
    "                    \"Accuracy\": results.get(\"accuracy\", \"N/A\"),\n",
    "                    \"Loss\": results.get(\"test_loss\", \"N/A\")\n",
    "                }\n",
    "                comparison_data.append(row)\n",
    "        \n",
    "        if comparison_data:\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            display(comparison_df)\n",
    "        \n",
    "        return comparison_data\n",
    "    \n",
    "    # Log our demo experiments\n",
    "    if 'synthetic_config' in locals() and synthetic_config:\n",
    "        log_experiment(\"Synthetic Demo\", synthetic_config.model_dump() if hasattr(synthetic_config, 'model_dump') else dict(synthetic_config))\n",
    "    \n",
    "    if 'real_config' in locals() and real_config:\n",
    "        log_experiment(\"Real Data Demo\", real_config.model_dump() if hasattr(real_config, 'model_dump') else dict(real_config))\n",
    "    \n",
    "    def save_experiments_log():\n",
    "        \"\"\"Save experiments to file.\"\"\"\n",
    "        if experiments_log:\n",
    "            log_file = Path(\"experiments_log.json\")\n",
    "            import json\n",
    "            with open(log_file, 'w') as f:\n",
    "                json.dump(experiments_log, f, indent=2, default=str)\n",
    "            print(f\"üíæ Saved experiments log to: {log_file}\")\n",
    "    \n",
    "    print(f\"üìä Current experiments logged: {len(experiments_log)}\")\n",
    "    if experiments_log:\n",
    "        compare_experiments()\n",
    "    \n",
    "    return log_experiment, compare_experiments, save_experiments_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99335569",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run interactive utilities\n",
    "create_interactive_model_explorer()\n",
    "data_quality_report = create_data_quality_analyzer()\n",
    "log_exp, compare_exp, save_exp_log = create_experiment_tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a56025",
   "metadata": {},
   "source": [
    "# üé® Section 10: Advanced Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769c55c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_advanced_visualizations():\n",
    "    \"\"\"Create advanced visualization tools for model analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® ADVANCED VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def plot_architecture_diagram():\n",
    "        \"\"\"Create a visual representation of the NexusFormer architecture.\"\"\"\n",
    "        print(\"üèóÔ∏è  Architecture Visualization\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        if loaded_model:\n",
    "            params = loaded_model.get_params()\n",
    "            input_dims = params.get('input_dimensions', [64, 32, 16])\n",
    "            embed_dim = params.get('architecture', {}).get('embed_dim', 64)\n",
    "            refinement_iterations = params.get('architecture', {}).get('refinement_iterations', 3)\n",
    "        else:\n",
    "            # Use example values\n",
    "            input_dims = [64, 32, 16]\n",
    "            embed_dim = 64\n",
    "            refinement_iterations = 3\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Input Dimensions\n",
    "        ax1 = axes[0, 0]\n",
    "        colors = sns.color_palette(\"viridis\", len(input_dims))\n",
    "        bars = ax1.bar(range(len(input_dims)), input_dims, color=colors)\n",
    "        ax1.set_title('Input Dimensions per Encoder')\n",
    "        ax1.set_xlabel('Encoder Index')\n",
    "        ax1.set_ylabel('Feature Count')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, dim in zip(bars, input_dims):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{dim}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Architecture Flow\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.set_title('NexusFormer Information Flow')\n",
    "        \n",
    "        # Create a simplified flow diagram\n",
    "        y_positions = np.arange(len(input_dims))\n",
    "        x_encoder = [0] * len(input_dims)\n",
    "        x_attention = [2] * len(input_dims)\n",
    "        x_fusion = [4]\n",
    "        \n",
    "        # Plot encoders\n",
    "        ax2.scatter(x_encoder, y_positions, s=200, c=colors, alpha=0.7, label='Encoders')\n",
    "        \n",
    "        # Plot attention mechanisms\n",
    "        ax2.scatter(x_attention, y_positions, s=150, c='red', marker='s', alpha=0.7, label='Cross-Attention')\n",
    "        \n",
    "        # Plot fusion\n",
    "        ax2.scatter(x_fusion, [len(input_dims)//2], s=300, c='gold', marker='D', label='Fusion Layer')\n",
    "        \n",
    "        # Draw connections\n",
    "        for i in y_positions:\n",
    "            # Encoder to attention\n",
    "            ax2.arrow(0.1, i, 1.7, 0, head_width=0.1, head_length=0.1, fc='gray', ec='gray', alpha=0.5)\n",
    "            # Attention to fusion\n",
    "            ax2.arrow(2.1, i, 1.5, len(input_dims)//2 - i - 0.1, head_width=0.1, head_length=0.1, fc='gray', ec='gray', alpha=0.5)\n",
    "        \n",
    "        ax2.set_xlim(-0.5, 5)\n",
    "        ax2.set_ylim(-0.5, len(input_dims) + 0.5)\n",
    "        ax2.set_xticks([0, 2, 4])\n",
    "        ax2.set_xticklabels(['Encoders', 'Cross-Attention', 'Fusion'])\n",
    "        ax2.set_yticks(range(len(input_dims)))\n",
    "        ax2.set_yticklabels([f'Table {i+1}' for i in range(len(input_dims))])\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Parameter Distribution\n",
    "        ax3 = axes[1, 0]\n",
    "        \n",
    "        # Estimate parameter distribution\n",
    "        encoder_params = sum(dim * embed_dim for dim in input_dims)\n",
    "        attention_params = len(input_dims) * embed_dim * embed_dim * 4  # Q,K,V,O\n",
    "        fusion_params = len(input_dims) * embed_dim * 64  # Rough estimate\n",
    "        \n",
    "        param_categories = ['Encoders', 'Cross-Attention', 'Fusion']\n",
    "        param_counts = [encoder_params, attention_params, fusion_params]\n",
    "        \n",
    "        wedges, texts, autotexts = ax3.pie(param_counts, labels=param_categories, autopct='%1.1f%%', startangle=90)\n",
    "        ax3.set_title('Parameter Distribution (Estimated)')\n",
    "        \n",
    "        # 4. Refinement Process\n",
    "        ax4 = axes[1, 1]\n",
    "        iterations = list(range(refinement_iterations + 1))\n",
    "        \n",
    "        # Simulate attention convergence (for illustration)\n",
    "        np.random.seed(42)\n",
    "        attention_changes = [1.0] + [np.exp(-i * 0.5) + np.random.normal(0, 0.1) for i in range(1, refinement_iterations + 1)]\n",
    "        attention_changes = [max(0, x) for x in attention_changes]  # Ensure non-negative\n",
    "        \n",
    "        ax4.plot(iterations, attention_changes, 'o-', linewidth=2, markersize=8)\n",
    "        ax4.set_title('Attention Refinement Process')\n",
    "        ax4.set_xlabel('Refinement Iteration')\n",
    "        ax4.set_ylabel('Attention Change Magnitude')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.set_ylim(0, max(attention_changes) * 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def plot_data_quality_heatmap():\n",
    "        \"\"\"Create a heatmap showing data quality metrics.\"\"\"\n",
    "        print(\"\\nüìä Data Quality Heatmap\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if data_quality_report:\n",
    "            # Convert quality report to matrix form\n",
    "            metrics = ['missing_percentage', 'numeric_columns', 'categorical_columns']\n",
    "            datasets = list(data_quality_report.keys())\n",
    "            \n",
    "            quality_matrix = []\n",
    "            for dataset in datasets:\n",
    "                row = []\n",
    "                for metric in metrics:\n",
    "                    value = data_quality_report[dataset].get(metric, 0)\n",
    "                    if metric == 'missing_percentage':\n",
    "                        # Invert missing percentage (lower is better)\n",
    "                        value = max(0, 100 - value)\n",
    "                    row.append(value)\n",
    "                quality_matrix.append(row)\n",
    "            \n",
    "            quality_df = pd.DataFrame(quality_matrix, index=datasets, columns=metrics)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(quality_df, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                       cbar_kws={'label': 'Quality Score'})\n",
    "            plt.title('Dataset Quality Overview')\n",
    "            plt.xlabel('Quality Metrics')\n",
    "            plt.ylabel('Datasets')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def create_feature_importance_plot():\n",
    "        \"\"\"Simulate and plot feature importance analysis.\"\"\"\n",
    "        print(\"\\nüéØ Feature Importance Analysis\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Simulate feature importance scores\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        if loaded_model:\n",
    "            params = loaded_model.get_params()\n",
    "            input_dims = params.get('input_dimensions', [])\n",
    "        else:\n",
    "            input_dims = [10, 8, 6]\n",
    "        \n",
    "        # Create mock feature importance data\n",
    "        all_features = []\n",
    "        all_importance = []\n",
    "        \n",
    "        for table_idx, dim in enumerate(input_dims):\n",
    "            table_name = f\"Table_{table_idx + 1}\"\n",
    "            for feat_idx in range(dim):\n",
    "                feature_name = f\"{table_name}_feat_{feat_idx + 1}\"\n",
    "                # Simulate decreasing importance with some randomness\n",
    "                importance = np.random.beta(2, 5) * (1 - feat_idx / dim)\n",
    "                \n",
    "                all_features.append(feature_name)\n",
    "                all_importance.append(importance)\n",
    "        \n",
    "        # Sort by importance\n",
    "        importance_data = sorted(zip(all_features, all_importance), key=lambda x: x[1], reverse=True)\n",
    "        features, importance = zip(*importance_data[:15])  # Top 15 features\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = []\n",
    "        for feat in features:\n",
    "            if 'Table_1' in feat:\n",
    "                colors.append('skyblue')\n",
    "            elif 'Table_2' in feat:\n",
    "                colors.append('lightgreen')\n",
    "            else:\n",
    "                colors.append('lightcoral')\n",
    "        \n",
    "        bars = plt.barh(range(len(features)), importance, color=colors)\n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.title('Top 15 Feature Importance (Simulated)')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        handles = [plt.Rectangle((0,0),1,1, color='skyblue', label='Table 1'),\n",
    "                  plt.Rectangle((0,0),1,1, color='lightgreen', label='Table 2'),\n",
    "                  plt.Rectangle((0,0),1,1, color='lightcoral', label='Table 3')]\n",
    "        plt.legend(handles=handles, loc='lower right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return list(zip(features, importance))\n",
    "    \n",
    "    # Create the visualizations\n",
    "    if 'matplotlib' in sys.modules:\n",
    "        arch_fig = plot_architecture_diagram()\n",
    "        plot_data_quality_heatmap()\n",
    "        feature_importance = create_feature_importance_plot()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Matplotlib not available for visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd25dc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run advanced visualizations\n",
    "create_advanced_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1cabd",
   "metadata": {},
   "source": [
    "# üîß Section 11: Troubleshooting & FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a8b01",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_troubleshooting_guide():\n",
    "    \"\"\"Create a comprehensive troubleshooting guide.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß TROUBLESHOOTING GUIDE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    problems_and_solutions = {\n",
    "        \"üö® Training Errors\": [\n",
    "            {\n",
    "                \"problem\": \"CUDA out of memory\",\n",
    "                \"solution\": \"Reduce batch_size in config, use smaller embed_dim, or train on CPU\",\n",
    "                \"example\": \"training: { batch_size: 16 }  # Instead of 64\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Model convergence issues\",\n",
    "                \"solution\": \"Lower learning rate, increase epochs, or reduce model complexity\",\n",
    "                \"example\": \"optimizer: { name: adam, lr: 1e-4 }  # Instead of 1e-3\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Validation loss not improving\",\n",
    "                \"solution\": \"Check for data leakage, reduce overfitting, or increase dataset size\",\n",
    "                \"example\": \"Add dropout, reduce refinement_iterations, or use early stopping\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"üóÇÔ∏è  Data Issues\": [\n",
    "            {\n",
    "                \"problem\": \"Primary key alignment fails\",\n",
    "                \"solution\": \"Ensure all datasets have the same primary key column with matching values\",\n",
    "                \"example\": \"All CSVs must have 'customer_id' column with identical IDs\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Missing values causing errors\", \n",
    "                \"solution\": \"Handle missing values before training or configure proper imputation\",\n",
    "                \"example\": \"df.fillna(df.mean()) or use more sophisticated imputation\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Inconsistent data types\",\n",
    "                \"solution\": \"Ensure numerical features are properly typed and categorical features encoded\",\n",
    "                \"example\": \"df['numeric_col'] = pd.to_numeric(df['numeric_col'], errors='coerce')\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"‚öôÔ∏è  Configuration Problems\": [\n",
    "            {\n",
    "                \"problem\": \"YAML parsing errors\",\n",
    "                \"solution\": \"Check indentation, quotes, and YAML syntax\",\n",
    "                \"example\": \"Use spaces (not tabs) for indentation, quote string values\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Model architecture mismatch\",\n",
    "                \"solution\": \"Ensure input_dims match your actual data dimensions\",\n",
    "                \"example\": \"Count features correctly after excluding primary key and target\"\n",
    "            },\n",
    "            {\n",
    "                \"problem\": \"Resource constraints\",\n",
    "                \"solution\": \"Scale down model size for local development\",\n",
    "                \"example\": \"Use embed_dim: 32, refinement_iterations: 2 for testing\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, issues in problems_and_solutions.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"\\n{i}. ‚ùì Problem: {issue['problem']}\")\n",
    "            print(f\"   üí° Solution: {issue['solution']}\")\n",
    "            print(f\"   üìù Example: {issue['example']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd69e39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_faq_section():\n",
    "    \"\"\"Create a frequently asked questions section.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùì FREQUENTLY ASKED QUESTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    faqs = [\n",
    "        {\n",
    "            \"q\": \"How does NexusFlow differ from traditional ML approaches?\",\n",
    "            \"a\": \"NexusFlow processes multiple tables simultaneously without flattening, using specialized transformers that communicate via cross-attention. This preserves relationships between tables and captures complex multi-hop dependencies.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"When should I use NexusFlow vs. standard approaches?\",\n",
    "            \"a\": \"Use NexusFlow when you have multiple related tables, complex relationships between entities, or when traditional feature engineering becomes too complex/lossy.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"How do I choose the right architecture parameters?\",\n",
    "            \"a\": \"Start with embed_dim=64 and refinement_iterations=3. Increase embed_dim for more complex data, increase refinement_iterations for stronger table interactions. Monitor validation performance.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Can I use different encoder types for different tables?\",\n",
    "            \"a\": \"Yes! Set transformer_type to 'standard', 'text', or 'timeseries' in your dataset configuration. Each table can have its own specialized encoder.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"How much data do I need for good results?\",\n",
    "            \"a\": \"Generally 1000+ aligned records minimum. More complex architectures need more data. The quality of relationships between tables is often more important than raw size.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Is NexusFlow suitable for production use?\",\n",
    "            \"a\": \"Yes, but ensure proper testing, monitoring, and MLOps practices. Start with smaller models in production and scale up based on performance requirements.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"How do I interpret the model's predictions?\",\n",
    "            \"a\": \"Use the visualize_flow() method to understand information flow between tables. Feature importance analysis and attention visualization help explain predictions.\"\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Can I add new tables to an existing model?\",\n",
    "            \"a\": \"Currently, you need to retrain with the new architecture. Future versions may support incremental table addition with transfer learning.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, faq in enumerate(faqs, 1):\n",
    "        print(f\"\\n{i}. ‚ùì {faq['q']}\")\n",
    "        print(f\"   üí¨ {faq['a']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559979d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_performance_optimization_tips():\n",
    "    \"\"\"Provide performance optimization tips.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö° PERFORMANCE OPTIMIZATION TIPS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimization_categories = {\n",
    "        \"üöÄ Training Speed\": [\n",
    "            \"Use GPU acceleration (CUDA) when available\",\n",
    "            \"Optimize batch_size for your hardware (try 32, 64, 128)\",\n",
    "            \"Use smaller embed_dim during development (32-64)\",\n",
    "            \"Reduce refinement_iterations for faster prototyping (1-2)\",\n",
    "            \"Use mixed precision training for larger models\",\n",
    "            \"Profile your code to identify bottlenecks\"\n",
    "        ],\n",
    "        \n",
    "        \"üíæ Memory Usage\": [\n",
    "            \"Reduce batch_size if getting OOM errors\",\n",
    "            \"Use gradient accumulation for effective larger batches\",\n",
    "            \"Clear unnecessary variables and use torch.cuda.empty_cache()\",\n",
    "            \"Process data in chunks if datasets are very large\",\n",
    "            \"Use appropriate data types (float32 instead of float64)\",\n",
    "            \"Monitor memory usage throughout training\"\n",
    "        ],\n",
    "        \n",
    "        \"üéØ Model Quality\": [\n",
    "            \"Start with synthetic data to validate architecture\",\n",
    "            \"Use cross-validation for robust evaluation\",\n",
    "            \"Implement early stopping to prevent overfitting\",\n",
    "            \"Experiment with different learning rates (1e-4 to 1e-2)\",\n",
    "            \"Try different optimizers (Adam, AdamW, SGD)\",\n",
    "            \"Use learning rate scheduling for better convergence\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Data Efficiency\": [\n",
    "            \"Ensure high-quality primary key alignment\",\n",
    "            \"Remove highly correlated or low-variance features\",\n",
    "            \"Use appropriate feature scaling within each table\",\n",
    "            \"Handle missing values strategically (not just fillna(0))\",\n",
    "            \"Consider feature selection techniques\",\n",
    "            \"Validate data quality before training\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in optimization_categories.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * 35)\n",
    "        for tip in tips:\n",
    "            print(f\"   ‚Ä¢ {tip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8084ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run troubleshooting sections\n",
    "create_troubleshooting_guide()\n",
    "create_faq_section()\n",
    "create_performance_optimization_tips()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07c1fe",
   "metadata": {},
   "source": [
    "# üéâ Section 12: Final Wrap-up & Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14077e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_final_summary():\n",
    "    \"\"\"Create the final comprehensive summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ NEXUSFLOW TUTORIAL COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüèÜ What You've Mastered:\")\n",
    "    mastery_areas = [\n",
    "        \"‚úÖ Project structure and initialization\",\n",
    "        \"‚úÖ Configuration management and best practices\", \n",
    "        \"‚úÖ Multi-table data preparation and alignment\",\n",
    "        \"‚úÖ Model training with both synthetic and real data\",\n",
    "        \"‚úÖ Architecture understanding and parameter tuning\",\n",
    "        \"‚úÖ Model evaluation and performance analysis\",\n",
    "        \"‚úÖ Inference and production deployment patterns\",\n",
    "        \"‚úÖ Troubleshooting and optimization techniques\",\n",
    "        \"‚úÖ Advanced visualization and model introspection\",\n",
    "        \"‚úÖ MLOps integration and experiment tracking\"\n",
    "    ]\n",
    "    \n",
    "    for area in mastery_areas:\n",
    "        print(f\"   {area}\")\n",
    "    \n",
    "    print(f\"\\nüóÇÔ∏è  Files Created in This Tutorial:\")\n",
    "    created_files = [\n",
    "        \"üìÅ nexusflow_demo/ - Sample project structure\",\n",
    "        \"üìÑ datasets/customers.csv - Sample customer data\",\n",
    "        \"üìÑ datasets/transactions.csv - Sample transaction data\", \n",
    "        \"üìÑ datasets/support.csv - Sample support data\",\n",
    "        \"üß† *.nxf - Trained model artifacts\",\n",
    "        \"üìà results/metrics.json - Training metrics log\",\n",
    "        \"üìã experiments_log.json - Experiment tracking data\"\n",
    "    ]\n",
    "    \n",
    "    for file_info in created_files:\n",
    "        print(f\"   {file_info}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready for Production:\")\n",
    "    production_checklist = [\n",
    "        \"üîß CLI commands: nexusflow init, train, validate\",\n",
    "        \"üèóÔ∏è  Architecture design principles and trade-offs\",\n",
    "        \"üìä Data quality assessment and improvement strategies\",\n",
    "        \"üéØ Model evaluation and performance monitoring\",\n",
    "        \"üîç Debugging and troubleshooting techniques\",\n",
    "        \"‚ö° Performance optimization for different scenarios\",\n",
    "        \"üõ°Ô∏è  Production deployment considerations and best practices\"\n",
    "    ]\n",
    "    \n",
    "    for item in production_checklist:\n",
    "        print(f\"   {item}\")\n",
    "    \n",
    "    print(f\"\\nüåü Key Takeaways:\")\n",
    "    takeaways = [\n",
    "        \"NexusFlow excels at learning from multiple related tables without data flattening\",\n",
    "        \"Cross-attention mechanisms enable sophisticated inter-table relationships\",\n",
    "        \"Start simple (synthetic data, small models) then scale up systematically\",\n",
    "        \"Configuration-driven approach enables reproducible experiments\",\n",
    "        \"Built-in MLOps integration supports production deployment\",\n",
    "        \"Visualization capabilities provide model interpretability\",\n",
    "        \"Community-focused design encourages contribution and extension\"\n",
    "    ]\n",
    "    \n",
    "    for takeaway in takeaways:\n",
    "        print(f\"   üí° {takeaway}\")\n",
    "    \n",
    "    print(f\"\\nüîó Connect with the Community:\")\n",
    "    community_info = [\n",
    "        \"üìñ Documentation: Explore the full project documentation\",\n",
    "        \"üêõ Issues: Report bugs or request features on GitHub\",\n",
    "        \"üí¨ Discussions: Join the community discussions\",\n",
    "        \"ü§ù Contributing: Submit pull requests and improvements\",\n",
    "        \"üìß Contact: Reach out to the development team\",\n",
    "        \"üéì Learning: Share your success stories and use cases\"\n",
    "    ]\n",
    "    \n",
    "    for info in community_info:\n",
    "        print(f\"   {info}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Thank you for exploring NexusFlow! üöÄ\")\n",
    "    print(\"Build amazing multi-transformer tabular ecosystems! üåü\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "create_final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99d1f3",
   "metadata": {},
   "source": [
    "Save this notebook's state for future reference"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
