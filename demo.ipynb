{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7a1d31",
   "metadata": {},
   "source": [
    "# NexusFlow Complete Demo: Multi-Table ML with Advanced Features\n",
    "\n",
    "Welcome to NexusFlow! This notebook demonstrates how to use NexusFlow's advanced multi-table machine learning capabilities with synthetic data. You'll learn how to train sophisticated models that can learn from multiple related datasets simultaneously.\n",
    "\n",
    "## What is NexusFlow?\n",
    "\n",
    "NexusFlow is a cutting-edge machine learning framework designed for **multi-table relational learning**. Unlike traditional ML approaches that work with single flattened datasets, NexusFlow can:\n",
    "\n",
    "- üîó **Learn from multiple related tables simultaneously**\n",
    "- üß† **Use advanced transformer architectures** (FT-Transformer, TabNet, Standard)\n",
    "- ‚ö° **Leverage FlashAttention** for efficient processing\n",
    "- üîÄ **Apply Mixture of Experts (MoE)** for complex patterns\n",
    "- üìä **Advanced preprocessing pipelines** with automatic feature detection\n",
    "- üéØ **Cross-contextual attention** to capture relationships between tables\n",
    "- üì¶ **Model optimization** with quantization and pruning\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7a05c",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ArkChaudhary/NexusFlow.git\n",
    "!pip install -r NexusFlow/requirements.txt\n",
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75635d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NexusFlow imports\n",
    "import sys\n",
    "sys.path.insert(0, './NexusFlow/src')\n",
    "from nexusflow.project_manager import ProjectManager\n",
    "from nexusflow.config import load_config_from_file\n",
    "from nexusflow.trainer.trainer import Trainer\n",
    "from nexusflow.api.model_api import load_model, extract_pytorch_model, create_optimized_artifact, ModelAPI\n",
    "from nexusflow.optimization.optimizer import optimize_model\n",
    "\n",
    "print(\"üöÄ NexusFlow environment ready!\")\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f06b76",
   "metadata": {},
   "source": [
    "## Section 2: Project Initialization & Data Preparation\n",
    "\n",
    "### Setting up the Project Structure\n",
    "\n",
    "NexusFlow follows a standardized project structure that keeps everything organized:\n",
    "\n",
    "```\n",
    "nexusflow_project/\n",
    "‚îú‚îÄ‚îÄ configs/          # Configuration files\n",
    "‚îú‚îÄ‚îÄ datasets/         # Raw data files\n",
    "‚îú‚îÄ‚îÄ models/          # Saved model artifacts\n",
    "‚îú‚îÄ‚îÄ results/         # Training results and logs\n",
    "‚îú‚îÄ‚îÄ notebooks/       # Jupyter notebooks\n",
    "‚îî‚îÄ‚îÄ src/            # Source code (if extending)\n",
    "```\n",
    "\n",
    "Let's create this structure and populate it with synthetic data for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project structure\n",
    "pm = ProjectManager()\n",
    "project_path = pm.init_project('nexusflow_colab_demo', force=True)\n",
    "datasets_path = os.path.join(project_path, 'datasets')\n",
    "os.makedirs(datasets_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "%cd nexusflow_colab_demo\n",
    "print(f\"üìÅ Project created at: {os.getcwd()}\")\n",
    "print(\"üìÇ Project structure:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965c3b",
   "metadata": {},
   "source": [
    "### Creating Synthetic Multi-Table Data\n",
    "\n",
    "For this tutorial, we'll create two related datasets:\n",
    "1. **`users.csv`** - User profile information (demographics, preferences)  \n",
    "2. **`transactions.csv`** - User transaction history (purchases, amounts, categories)\n",
    "\n",
    "Our goal will be to predict user **churn risk** based on both profile and transaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Users Dataset\n",
    "n_users = 1000\n",
    "user_ids = range(1, n_users + 1)\n",
    "\n",
    "users_data = {\n",
    "    'user_id': user_ids,\n",
    "    'age': np.random.normal(35, 12, n_users).astype(int).clip(18, 80),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_users).astype(int).clip(20000, 200000),\n",
    "    'account_tenure_months': np.random.exponential(24, n_users).astype(int).clip(1, 120),\n",
    "    'num_products': np.random.poisson(2.5, n_users).clip(1, 8),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_users, \n",
    "                              p=[0.2, 0.25, 0.2, 0.15, 0.2]),\n",
    "    'subscription_tier': np.random.choice(['Basic', 'Premium', 'Enterprise'], n_users, \n",
    "                                        p=[0.5, 0.35, 0.15]),\n",
    "    'support_tickets': np.random.poisson(1.2, n_users),\n",
    "}\n",
    "\n",
    "users_df = pd.DataFrame(users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0992d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Transactions Dataset\n",
    "n_transactions = 5000\n",
    "transaction_user_ids = np.random.choice(user_ids, n_transactions, \n",
    "                                      p=np.random.dirichlet(np.ones(n_users)))\n",
    "\n",
    "transactions_data = {\n",
    "    'user_id': transaction_user_ids,\n",
    "    'transaction_amount': np.random.lognormal(4, 1, n_transactions).clip(10, 5000),\n",
    "    'transaction_frequency': np.random.exponential(2, n_transactions).clip(0.1, 20),\n",
    "    'days_since_last_transaction': np.random.exponential(7, n_transactions).astype(int).clip(0, 365),\n",
    "    'payment_method': np.random.choice(['Credit', 'Debit', 'Digital', 'Cash'], n_transactions,\n",
    "                                     p=[0.4, 0.3, 0.25, 0.05]),\n",
    "    'transaction_category': np.random.choice(['Retail', 'Services', 'Entertainment', 'Food', 'Other'], \n",
    "                                           n_transactions, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "    'merchant_rating': np.random.beta(8, 2, n_transactions) * 5,  # Skewed toward high ratings\n",
    "}\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2908a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic churn labels with realistic patterns\n",
    "# Higher churn probability for: low income, few products, high support tickets, low transaction frequency\n",
    "churn_probability = (\n",
    "    0.1 +  # Base rate\n",
    "    0.15 * (users_df['income'] < 40000) +  # Low income\n",
    "    0.1 * (users_df['num_products'] == 1) +  # Single product\n",
    "    0.05 * (users_df['support_tickets'] > 2) +  # Many support tickets\n",
    "    0.1 * (users_df['account_tenure_months'] < 6)  # New accounts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some transaction-based features for churn\n",
    "user_tx_summary = transactions_df.groupby('user_id').agg({\n",
    "    'transaction_frequency': 'mean',\n",
    "    'days_since_last_transaction': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "users_df = users_df.merge(user_tx_summary, on='user_id', how='left')\n",
    "users_df['transaction_frequency'] = users_df['transaction_frequency'].fillna(0)\n",
    "users_df['days_since_last_transaction'] = users_df['days_since_last_transaction'].fillna(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad962477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust churn probability based on transaction patterns\n",
    "churn_probability += 0.2 * (users_df['transaction_frequency'] < 0.5)  # Low frequency\n",
    "churn_probability += 0.15 * (users_df['days_since_last_transaction'] > 90)  # Long absence\n",
    "\n",
    "users_df['churn_risk'] = np.random.binomial(1, churn_probability.clip(0, 0.8), n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop helper columns used for churn generation\n",
    "users_df = users_df.drop(['transaction_frequency', 'days_since_last_transaction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0294ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "users_df.to_csv('datasets/users.csv', index=False)\n",
    "transactions_df.to_csv('datasets/transactions.csv', index=False)\n",
    "\n",
    "print(\"üìä Synthetic datasets created successfully!\")\n",
    "print(f\"üë• Users dataset: {len(users_df):,} records, {len(users_df.columns)} columns\")\n",
    "print(f\"üí≥ Transactions dataset: {len(transactions_df):,} records, {len(transactions_df.columns)} columns\")\n",
    "print(f\"üéØ Churn rate: {users_df['churn_risk'].mean():.1%}\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nüìã Users dataset preview:\")\n",
    "print(users_df.head())\n",
    "print(\"\\nüìã Transactions dataset preview:\")  \n",
    "print(transactions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9768d51",
   "metadata": {},
   "source": [
    "## Section 3: Configuration\n",
    "\n",
    "### Configuring the Model\n",
    "\n",
    "NexusFlow is configured using a single YAML file that specifies:\n",
    "- **Project settings** (name, primary key, target variable)\n",
    "- **Dataset configurations** (transformer types, complexity levels)\n",
    "- **Architecture settings** (embedding dimensions, refinement iterations)\n",
    "- **Advanced features** (MoE, FlashAttention, preprocessing)\n",
    "- **Training parameters** (batch size, epochs, optimization)\n",
    "\n",
    "Let's create a sophisticated configuration that showcases NexusFlow's advanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configs/config.yaml\n",
    "project_name: \"nexusflow_churn_prediction\"\n",
    "primary_key: \"user_id\"\n",
    "\n",
    "target:\n",
    "  target_column: \"churn_risk\"\n",
    "  target_table: \"users.csv\"\n",
    "\n",
    "# Dataset configurations with different transformer types\n",
    "datasets:\n",
    "  - name: \"users.csv\"\n",
    "    transformer_type: \"ft_transformer\"  # Feature Tokenizer Transformer for tabular data\n",
    "    complexity: \"medium\"\n",
    "    context_weight: 1.0\n",
    "    categorical_columns: [\"region\", \"subscription_tier\"]\n",
    "    numerical_columns: [\"age\", \"income\", \"account_tenure_months\", \"num_products\", \"support_tickets\"]\n",
    "  \n",
    "  - name: \"transactions.csv\"\n",
    "    transformer_type: \"standard\"  # Standard transformer with FlashAttention\n",
    "    complexity: \"medium\"\n",
    "    context_weight: 0.8\n",
    "    categorical_columns: [\"payment_method\", \"transaction_category\"]\n",
    "    numerical_columns: [\"transaction_amount\", \"transaction_frequency\", \"days_since_last_transaction\", \"merchant_rating\"]\n",
    "\n",
    "# Architecture configuration\n",
    "architecture:\n",
    "  global_embed_dim: 128\n",
    "  refinement_iterations: 4\n",
    "  use_moe: true              # Enable Mixture of Experts\n",
    "  num_experts: 6             # Number of expert networks\n",
    "  use_flash_attn: true       # Enable FlashAttention optimization\n",
    "  top_k_contexts: 3          # Limit cross-attention to top-3 most relevant contexts\n",
    "\n",
    "# Training configuration\n",
    "training:\n",
    "  batch_size: 32\n",
    "  epochs: 15\n",
    "  \n",
    "  optimizer:\n",
    "    name: \"adam\"\n",
    "    lr: 0.001\n",
    "    weight_decay: 0.0001\n",
    "  \n",
    "  split_config:\n",
    "    test_size: 0.2\n",
    "    validation_size: 0.2\n",
    "    randomize: true\n",
    "  \n",
    "  # Synthetic data options\n",
    "  use_synthetic: false\n",
    "  synthetic:\n",
    "    n_samples: 256\n",
    "    feature_dim: 5\n",
    "  \n",
    "  # Advanced preprocessing\n",
    "  use_advanced_preprocessing: true\n",
    "  auto_detect_types: true\n",
    "  \n",
    "  # Training optimizations\n",
    "  early_stopping: true\n",
    "  patience: 5\n",
    "  gradient_clipping: 1.0\n",
    "\n",
    "# MLOps configuration\n",
    "mlops:\n",
    "  logging_provider: \"stdout\"\n",
    "  experiment_name: \"churn_prediction_demo\"\n",
    "  log_attention_patterns: false      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Configuration file created successfully!\")\n",
    "print(\"üìù Key configuration highlights:\")\n",
    "print(\"   üîÑ FT-Transformer for users table (advanced tabular processing)\")  \n",
    "print(\"   ‚ö° Standard transformer with FlashAttention for transactions\")\n",
    "print(\"   üîÄ Mixture of Experts enabled (6 experts)\")\n",
    "print(\"   üß† Advanced preprocessing with auto-detection\")\n",
    "print(\"   üéØ Binary classification: churn risk prediction\")\n",
    "print(\"   üìä 4 refinement iterations for cross-table learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8130488",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "### Let's Train!\n",
    "\n",
    "With our data and configuration ready, training a sophisticated multi-table model is remarkably simple. NexusFlow handles all the complexity behind the scenes:\n",
    "\n",
    "- **Automatic data loading and preprocessing**\n",
    "- **Cross-table relationship modeling**\n",
    "- **Advanced transformer architectures**\n",
    "- **Mixture of Experts routing**\n",
    "- **FlashAttention optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0054b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and initialize trainer\n",
    "config = load_config_from_file('configs/config.yaml')\n",
    "trainer = Trainer(config, work_dir='.')\n",
    "\n",
    "print(\"üéØ Starting NexusFlow training with advanced features...\")\n",
    "#print(f\"üìä Model architecture: {config.architecture.num_experts} MoE experts, {config.architecture.global_embed_dim}D embeddings\")\n",
    "print(f\"‚ö° Optimizations: FlashAttention={config.architecture.use_flash_attn}, MoE={config.architecture.use_moe}\")\n",
    "print(f\"üîÑ Cross-table refinement: {config.architecture.refinement_iterations} iterations\")\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nüéâ Training completed successfully!\")\n",
    "print(\"üìÅ Artifacts created:\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction.nxf (complete model artifact)\")\n",
    "print(\"   üíæ best_model.pt (PyTorch checkpoint)\")\n",
    "print(\"   üìä results/training_history.json (training metrics)\")\n",
    "print(\"   üîß preprocessing/ (preprocessing artifacts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81035c",
   "metadata": {},
   "source": [
    "## Section 5: Using the Trained Model\n",
    "\n",
    "### Model Loading and Inference\n",
    "\n",
    "Our training produced a complete `.nxf` model artifact that contains:\n",
    "- **The trained NexusFormer model**\n",
    "- **All preprocessing transformations**\n",
    "- **Model configuration and metadata**\n",
    "- **Performance metrics**\n",
    "\n",
    "Let's load this artifact and make predictions on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model artifact\n",
    "model = load_model('nexusflow_churn_prediction.nxf')\n",
    "\n",
    "print(\"üì¶ Model artifact loaded successfully!\")\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Get detailed model parameters\n",
    "params = model.get_params()\n",
    "print(f\"\\nüîç Model Details:\")\n",
    "print(f\"   Total Parameters: {params['total_parameters']:,}\")\n",
    "print(f\"   Architecture: {params['model_class']} with {params['architecture']['num_encoders']} encoders\")\n",
    "print(f\"   Training Accuracy: {params['training_info']['best_val_metric']:.4f}\")\n",
    "print(f\"   Preprocessing: {'Enabled' if params.get('phase_2_features', {}).get('advanced_preprocessing', False) else 'Basic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271983cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new sample data for inference (simulating new users and their transactions)\n",
    "np.random.seed(123)  # Different seed for new data\n",
    "\n",
    "# New users data\n",
    "new_users = pd.DataFrame({\n",
    "    'user_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'age': [28, 45, 36, 52, 31],\n",
    "    'income': [45000, 85000, 62000, 120000, 38000],\n",
    "    'account_tenure_months': [3, 24, 12, 48, 6],\n",
    "    'num_products': [1, 3, 2, 5, 1],\n",
    "    'region': ['North', 'South', 'East', 'West', 'Central'],\n",
    "    'subscription_tier': ['Basic', 'Premium', 'Basic', 'Enterprise', 'Basic'],\n",
    "    'support_tickets': [2, 0, 1, 0, 3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New transactions data\n",
    "new_transactions = pd.DataFrame({\n",
    "    'user_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'transaction_amount': [45.0, 89.99, 234.50, 1250.00, 15.99],\n",
    "    'transaction_frequency': [0.5, 1.8, 1.2, 3.5, 0.2],\n",
    "    'days_since_last_transaction': [45, 7, 14, 1, 89],\n",
    "    'payment_method': ['Credit', 'Digital', 'Credit', 'Credit', 'Cash'],\n",
    "    'transaction_category': ['Retail', 'Entertainment', 'Retail', 'Services', 'Other'],\n",
    "    'merchant_rating': [4.2, 3.8, 4.5, 4.9, 3.2]\n",
    "})\n",
    "\n",
    "print(\"üë§ New sample data created:\")\n",
    "print(new_users[['user_id', 'age', 'income', 'subscription_tier', 'support_tickets']])\n",
    "print(f\"\\nüí≥ Associated transactions: {len(new_transactions)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the multi-table format\n",
    "prediction_data = {\n",
    "    'users.csv': new_users,\n",
    "    'transactions.csv': new_transactions  \n",
    "}\n",
    "\n",
    "# Get predictions (churn probabilities)\n",
    "predictions = model.predict(prediction_data)\n",
    "\n",
    "print(\"üéØ Churn Risk Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_df = new_users[['user_id', 'age', 'income', 'subscription_tier']].copy()\n",
    "results_df['churn_probability'] = predictions\n",
    "results_df['risk_level'] = results_df['churn_probability'].apply(\n",
    "    lambda x: 'High' if x > 0.7 else ('Medium' if x > 0.4 else 'Low')\n",
    ")\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"User {row['user_id']:4d}: {row['churn_probability']:.3f} ({row['risk_level']:6s}) - \"\n",
    "          f\"Age {row['age']}, {row['subscription_tier']:10s}, ${row['income']:,}\")\n",
    "\n",
    "print(f\"\\nüìä Risk Distribution:\")\n",
    "print(f\"   High Risk (>70%):   {sum(results_df['risk_level'] == 'High')} users\")\n",
    "print(f\"   Medium Risk (40-70%): {sum(results_df['risk_level'] == 'Medium')} users\") \n",
    "print(f\"   Low Risk (<40%):    {sum(results_df['risk_level'] == 'Low')} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c0b28",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "NexusFlow includes state-of-the-art model optimization techniques to reduce model size and improve inference speed without sacrificing accuracy. Let's demonstrate both quantization and pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80720b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:09.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m562\u001b[0m - \u001b[1mLoading model with: encoder_type=standard, use_moe=True, num_experts=6, use_flash_attn=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:09.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.model.nexus_former\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m553\u001b[0m - \u001b[1mEnhanced NexusFormer initialized: 2 standard encoders, 4 iterations, MoE=True, FlashAttn=True\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:09.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m577\u001b[0m - \u001b[1mNexusFlow model artifact loaded from: nexusflow_churn_prediction.nxf\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:09.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m579\u001b[0m - \u001b[1mLoaded with preprocessors for: ['users.csv', 'transactions.csv']\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:09.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:09.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessors available for: ['users.csv', 'transactions.csv']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Model Optimization Demonstration\n",
      "==================================================\n",
      "üìä Original Model:\n",
      "   Parameters: 2,126,873\n",
      "   Size: 8.11 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the original trained model\n",
    "original_artifact = load_model('nexusflow_churn_prediction.nxf')\n",
    "\n",
    "print(\"üîß Model Optimization Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "params_info = original_artifact.get_params()\n",
    "original_params = params_info['total_parameters']\n",
    "original_size_mb = original_params * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"üìä Original Model:\")\n",
    "print(f\"   Parameters: {original_params:,}\")\n",
    "print(f\"   Size: {original_size_mb:.2f} MB\")\n",
    "\n",
    "# Extract the actual PyTorch model for optimization\n",
    "actual_model = extract_pytorch_model(original_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fc0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:13.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1müöÄ Starting model optimization with method: quantization\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1müîß Starting dynamic quantization...\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1m   Original model size: 8.11 MB\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1m‚úÖ Dynamic quantization complete:\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1m   Quantized model size: 0.27 MB\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1m   Size reduction: 96.7%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m   Quantization time: 0.09s\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mquantize_model\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m   Target layers: 71 Linear layers\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1müéØ Model optimization summary:\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m   Method: dynamic_quantization\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m   Parameter reduction: 96.7%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1m   Size reduction: 96.7%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:13.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1m   Total time: 0.10s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Applying Dynamic Quantization...\n",
      "\n",
      "üìà Quantization Results:\n",
      "   Method: dynamic_quantization\n",
      "   Size Reduction: 96.7%\n",
      "   Parameter Reduction: 96.7%\n",
      "   Original Size: 8.11 MB\n",
      "   Optimized Size: 0.27 MB\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate quantization\n",
    "print(\"\\n‚ö° Applying Dynamic Quantization...\")\n",
    "quantized_pytorch_model, quant_metadata = optimize_model(actual_model, method='quantization')\n",
    "\n",
    "print(f\"\\nüìà Quantization Results:\")\n",
    "print(f\"   Method: {quant_metadata['method']}\")\n",
    "print(f\"   Size Reduction: {quant_metadata['size_reduction']:.1%}\")\n",
    "print(f\"   Parameter Reduction: {quant_metadata['parameter_reduction']:.1%}\")\n",
    "print(f\"   Original Size: {quant_metadata['original_size_mb']:.2f} MB\")\n",
    "print(f\"   Optimized Size: {quant_metadata['optimized_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b519aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:14.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1müöÄ Starting model optimization with method: pruning\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m‚úÇÔ∏è  Starting global unstructured pruning (amount=30.0%)...\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m   Original parameters: 2,126,873\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1m   Original model size: 8.11 MB\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1m   Found 72 Linear layers to prune\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è  Applying Global Unstructured Pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:14.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1m‚úÖ Global unstructured pruning complete:\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1m   Pruned parameters: 2,126,873\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1m   Actual sparsity: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1m   Size reduction: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1m   New model size: 8.11 MB\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1m   Pruning time: 0.24s\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36mprune_model\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1m   Pruned layers: 72\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1müéØ Model optimization summary:\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1m   Method: global_unstructured_pruning\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1m   Parameter reduction: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1m   Size reduction: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:14.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.optimization.optimizer\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1m   Total time: 0.25s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Pruning Results:\n",
      "   Method: global_unstructured_pruning\n",
      "   Pruning Amount: 30%\n",
      "   Actual Size Reduction: 0.0%\n",
      "   Parameter Reduction: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate pruning\n",
    "print(\"\\n‚úÇÔ∏è  Applying Global Unstructured Pruning...\")\n",
    "pruned_pytorch_model, prune_metadata = optimize_model(actual_model, method='pruning', amount=0.3)\n",
    "\n",
    "print(f\"\\nüìà Pruning Results:\")\n",
    "print(f\"   Method: {prune_metadata['method']}\")\n",
    "print(f\"   Pruning Amount: 30%\")\n",
    "print(f\"   Actual Size Reduction: {prune_metadata['size_reduction']:.1%}\")\n",
    "print(f\"   Parameter Reduction: {prune_metadata['parameter_reduction']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:50:17.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessors available for: ['users.csv', 'transactions.csv']\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessors available for: ['users.csv', 'transactions.csv']\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mSaving optimized model with dynamic_quantization\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1m  Size reduction: 96.7%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m509\u001b[0m - \u001b[1m  Parameter reduction: 96.7%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m518\u001b[0m - \u001b[1mNexusFlow model artifact saved to: nexusflow_churn_prediction_quantized.nxf\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessors available for: ['users.csv', 'transactions.csv']\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mNexusFlow model artifact initialized with 2 input dimensions\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mPreprocessors available for: ['users.csv', 'transactions.csv']\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m507\u001b[0m - \u001b[1mSaving optimized model with global_unstructured_pruning\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m508\u001b[0m - \u001b[1m  Size reduction: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m509\u001b[0m - \u001b[1m  Parameter reduction: 0.0%\u001b[0m\n",
      "\u001b[32m2025-08-31 19:50:17.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m518\u001b[0m - \u001b[1mNexusFlow model artifact saved to: nexusflow_churn_prediction_pruned.nxf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving Optimized Models...\n",
      "‚úÖ Optimized model artifacts saved:\n",
      "   üì¶ nexusflow_churn_prediction_quantized.nxf\n",
      "   üì¶ nexusflow_churn_prediction_pruned.nxf\n"
     ]
    }
   ],
   "source": [
    "# Create optimized artifacts properly\n",
    "print(\"\\nüíæ Saving Optimized Models...\")\n",
    "\n",
    "# Create quantized artifact\n",
    "quantized_artifact = create_optimized_artifact(original_artifact, quantized_pytorch_model, quant_metadata)\n",
    "ModelAPI(quantized_pytorch_model, quantized_artifact.meta).save('nexusflow_churn_prediction_quantized.nxf')\n",
    "\n",
    "# Create pruned artifact  \n",
    "pruned_artifact = create_optimized_artifact(original_artifact, pruned_pytorch_model, prune_metadata)\n",
    "ModelAPI(pruned_pytorch_model, pruned_artifact.meta).save('nexusflow_churn_prediction_pruned.nxf')\n",
    "\n",
    "print(\"‚úÖ Optimized model artifacts saved:\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction_quantized.nxf\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction_pruned.nxf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Model Comparison:\n",
      "   Original:  8.19 MB (100%)\n",
      "   Quantized: 2.39 MB (29.2%)\n",
      "   Pruned:    8.19 MB (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Compare all model sizes\n",
    "import os\n",
    "original_size = os.path.getsize('nexusflow_churn_prediction.nxf') / (1024 * 1024)\n",
    "quantized_size = os.path.getsize('nexusflow_churn_prediction_quantized.nxf') / (1024 * 1024)  \n",
    "pruned_size = os.path.getsize('nexusflow_churn_prediction_pruned.nxf') / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä Final Model Comparison:\")\n",
    "print(f\"   Original:  {original_size:.2f} MB (100%)\")\n",
    "print(f\"   Quantized: {quantized_size:.2f} MB ({quantized_size/original_size*100:.1f}%)\")\n",
    "print(f\"   Pruned:    {pruned_size:.2f} MB ({pruned_size/original_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b90a41",
   "metadata": {},
   "source": [
    "## Section 6: Advanced Features Deep Dive\n",
    "\n",
    "### Cross-Contextual Attention Analysis\n",
    "\n",
    "One of NexusFormer's key innovations is its cross-contextual attention mechanism, which learns relationships between different tables. Let's examine how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 19:54:34.784\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m421\u001b[0m - \u001b[33m\u001b[1mInteractive visualization not yet implemented\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m422\u001b[0m - \u001b[1mModel architecture summary:\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m423\u001b[0m - \u001b[1m  - 2 contextual encoders\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m424\u001b[0m - \u001b[1m  - Input dimensions: [7, 6]\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m425\u001b[0m - \u001b[1m  - Refinement iterations: 4\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m426\u001b[0m - \u001b[1m  - Total parameters: 2126873\u001b[0m\n",
      "\u001b[32m2025-08-31 19:54:34.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnexusflow.api.model_api\u001b[0m:\u001b[36mvisualize_flow\u001b[0m:\u001b[36m427\u001b[0m - \u001b[1m  - Preprocessing: Available\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† NexusFormer Architecture Analysis\n",
      "==================================================\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   Architecture Type: NexusFormer\n",
      "   Number of Contextual Encoders: 2\n",
      "   Input Dimensions: [7, 6]\n",
      "   Refinement Iterations: 4\n",
      "\n",
      "üîÑ Learning Process:\n",
      "   1. Each table (users, transactions) is processed by specialized encoders\n",
      "   2. Cross-contextual attention learns relationships between tables\n",
      "   3. 4 refinement iterations enhance representations\n",
      "   4. Final fusion layer combines all contextual information\n",
      "   5. Prediction head outputs churn probability\n"
     ]
    }
   ],
   "source": [
    "# Visualize model architecture and attention patterns\n",
    "print(\"üß† NexusFormer Architecture Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model = original_artifact\n",
    "\n",
    "architecture_info = model.visualize_flow()\n",
    "\n",
    "print(f\"üèóÔ∏è  Model Architecture:\")\n",
    "print(f\"   Architecture Type: {architecture_info['architecture']}\")\n",
    "print(f\"   Number of Contextual Encoders: {architecture_info['num_encoders']}\")\n",
    "print(f\"   Input Dimensions: {architecture_info['input_dims']}\")\n",
    "print(f\"   Refinement Iterations: {architecture_info['refinement_iterations']}\")\n",
    "\n",
    "print(f\"\\nüîÑ Learning Process:\")\n",
    "print(f\"   1. Each table (users, transactions) is processed by specialized encoders\")\n",
    "print(f\"   2. Cross-contextual attention learns relationships between tables\")\n",
    "print(f\"   3. {architecture_info['refinement_iterations']} refinement iterations enhance representations\")\n",
    "print(f\"   4. Final fusion layer combines all contextual information\")\n",
    "print(f\"   5. Prediction head outputs churn probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29514a76",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Basic preprocessing was used (fillna-based)\n"
     ]
    }
   ],
   "source": [
    "# Examine the preprocessing pipeline that was automatically applied\n",
    "if hasattr(model, 'get_params'):\n",
    "    params = model.get_params()\n",
    "    if 'phase_2_features' in params and params['phase_2_features']['advanced_preprocessing']:\n",
    "        print(\"\\nüîß Advanced Preprocessing Pipeline Applied:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        preprocessing_datasets = params['phase_2_features']['preprocessor_datasets']\n",
    "        print(f\"‚úÖ Datasets processed: {preprocessing_datasets}\")\n",
    "        print(\"üîç Automatic feature type detection enabled\")\n",
    "        print(\"üìä Advanced categorical encoding applied\")\n",
    "        print(\"üìà Numerical feature normalization applied\")\n",
    "        print(\"üö´ Missing value imputation handled\")\n",
    "        \n",
    "        print(f\"\\nüìã Feature Processing Summary:\")\n",
    "        for dataset in preprocessing_datasets:\n",
    "            print(f\"   {dataset.title()} table: Categorical & numerical features automatically detected\")\n",
    "    else:\n",
    "        print(\"\\nüí° Basic preprocessing was used (fillna-based)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec4e50",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation & Insights\n",
    "\n",
    "### Comprehensive Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843eb6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training Performance Analysis\n",
      "==================================================\n",
      "üèÜ Best Model: Epoch 14 with validation loss 0.073231\n",
      "üìä Total Epochs: 15\n",
      "‚è∞ Early Stopping: No\n",
      "üîß Advanced Preprocessing: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load training history and analyze model performance\n",
    "with open('results/training_history.json', 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "history = training_data['training_history']\n",
    "best_epoch = training_data['best_epoch']\n",
    "best_metric = training_data['best_metric']\n",
    "\n",
    "print(\"üìà Training Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üèÜ Best Model: Epoch {best_epoch} with validation loss {best_metric:.6f}\")\n",
    "print(f\"üìä Total Epochs: {len(history)}\")\n",
    "print(f\"‚è∞ Early Stopping: {'Yes' if training_data.get('early_stopped', False) else 'No'}\")\n",
    "print(f\"üîß Advanced Preprocessing: {'Yes' if training_data.get('preprocessing_enabled', False) else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a357c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìâ Training Progress:\n",
      "   Initial Training Loss: 0.099689\n",
      "   Final Training Loss: 0.075091\n",
      "   Initial Validation Loss: 0.099898\n",
      "   Best Validation Loss: 0.073231\n"
     ]
    }
   ],
   "source": [
    "# Extract metrics for visualization\n",
    "epochs = [h['epoch'] for h in history]\n",
    "train_losses = [h['train_loss'] for h in history]\n",
    "val_losses = [h.get('val_loss', None) for h in history if h.get('val_loss') is not None]\n",
    "\n",
    "print(f\"\\nüìâ Training Progress:\")\n",
    "print(f\"   Initial Training Loss: {train_losses[0]:.6f}\")\n",
    "print(f\"   Final Training Loss: {train_losses[-1]:.6f}\")\n",
    "if val_losses:\n",
    "    print(f\"   Initial Validation Loss: {val_losses[0]:.6f}\")\n",
    "    print(f\"   Best Validation Loss: {min(val_losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0967bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Model Performance Metrics:\n",
      "   Best Validation Metric: 0.0732\n",
      "   Best Epoch: 14\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation metrics\n",
    "eval_results = model.evaluate()\n",
    "if eval_results:\n",
    "    print(f\"\\nüéØ Model Performance Metrics:\")\n",
    "    for metric, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"   {metric.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08d4b2",
   "metadata": {},
   "source": [
    "## Section 8: Conclusion & Next Steps\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the NexusFlow tutorial and experienced the power of advanced multi-table machine learning! Here's what you accomplished:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315f540",
   "metadata": {},
   "source": [
    "**‚úÖ What You Built:**\n",
    "- **Multi-table ML model** using users and transactions data\n",
    "- **Advanced transformer architectures** (FT-Transformer + Standard)\n",
    "- **Mixture of Experts** for handling complex patterns\n",
    "- **FlashAttention optimization** for efficient processing\n",
    "- **Cross-contextual learning** between related tables\n",
    "- **Advanced preprocessing** with automatic feature detection\n",
    "- **Model optimization** with quantization and pruning\n",
    "\n",
    "**üöÄ Key Takeaways:**\n",
    "1. **Multi-table learning** can capture complex relationships that single-table models miss\n",
    "2. **Advanced architectures** like MoE and FlashAttention provide both performance and efficiency\n",
    "3. **Automatic preprocessing** reduces manual feature engineering overhead\n",
    "4. **Model optimization** can dramatically reduce deployment costs\n",
    "5. **NexusFlow's unified API** makes complex ML accessible\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "```python\n",
    "print(\"üéØ Next Steps & Advanced Usage:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. üìä Try with your own multi-table datasets\")\n",
    "print(\"2. üîß Experiment with different transformer types (TabNet, etc.)\")  \n",
    "print(\"3. üéõÔ∏è  Tune hyperparameters for your specific domain\")\n",
    "print(\"4. üîÄ Explore more MoE configurations\")\n",
    "print(\"5. üìà Scale to larger datasets and more tables\")\n",
    "print(\"6. üåê Deploy optimized models to production\")\n",
    "print(\"7. üìö Check the NexusFlow documentation for advanced features\")\n",
    "\n",
    "print(f\"\\nüí° Pro Tips:\")\n",
    "print(\"   ‚Ä¢ Use 'ft_transformer' for tables with mixed categorical/numerical features\")\n",
    "print(\"   ‚Ä¢ Enable MoE for datasets with complex, diverse patterns\")\n",
    "print(\"   ‚Ä¢ Apply model optimization before production deployment\")\n",
    "print(\"   ‚Ä¢ Experiment with refinement_iterations for better cross-table learning\")\n",
    "print(\"   ‚Ä¢ Monitor attention patterns for model interpretability\")\n",
    "\n",
    "print(f\"\\nüìÅ Your artifacts are ready for production:\")\n",
    "print(\"   üì¶ nexusflow_churn_prediction.nxf (original model)\")\n",
    "print(\"   ‚ö° nexusflow_churn_prediction_quantized.nxf (optimized for speed)\")\n",
    "print(\"   üóúÔ∏è  nexusflow_churn_prediction_pruned.nxf (optimized for size)\")\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Documentation:** Check the NexusFlow docs for comprehensive guides\n",
    "- **GitHub:** Explore the source code and contribute  \n",
    "- **Community:** Join discussions and share your experiences\n",
    "- **Examples:** Browse more advanced use cases and tutorials\n",
    "\n",
    "**Happy modeling with NexusFlow!** üöÄ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
